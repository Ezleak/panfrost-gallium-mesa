diff a/src/gallium/drivers/panfrost/pan_cmdstream.c b/src/gallium/drivers/panfrost/pan_cmdstream.c	(rejected hunks)
@@ -23,158 +24,127 @@
  * SOFTWARE.
  */
 
+#include "gallium/auxiliary/util/u_blend.h"
+#include "pipe/p_defines.h"
+#include "pipe/p_state.h"
 #include "util/macros.h"
-#include "util/u_prim.h"
-#include "util/u_vbuf.h"
-#include "util/u_helpers.h"
 #include "util/u_draw.h"
+#include "util/u_helpers.h"
 #include "util/u_memory.h"
-#include "pipe/p_defines.h"
-#include "pipe/p_state.h"
-#include "gallium/auxiliary/util/u_blend.h"
+#include "util/u_prim.h"
+#include "util/u_sample_positions.h"
+#include "util/u_vbuf.h"
+#include "util/u_viewport.h"
+
+#include "decode.h"
 
 #include "genxml/gen_macros.h"
 
-#include "pan_pool.h"
-#include "pan_bo.h"
+#include "pan_afbc_cso.h"
 #include "pan_blend.h"
+#include "pan_blitter.h"
+#include "pan_bo.h"
+#include "pan_cmdstream.h"
 #include "pan_context.h"
+#include "pan_indirect_dispatch.h"
+#include "pan_jm.h"
 #include "pan_job.h"
+#include "pan_pool.h"
 #include "pan_shader.h"
 #include "pan_texture.h"
 #include "pan_util.h"
-#include "pan_indirect_draw.h"
-#include "pan_indirect_dispatch.h"
-#include "pan_blitter.h"
-
-#define PAN_GPU_INDIRECTS (PAN_ARCH == 7)
-
-struct panfrost_rasterizer {
-        struct pipe_rasterizer_state base;
-
-#if PAN_ARCH <= 7
-        /* Partially packed RSD words */
-        struct mali_multisample_misc_packed multisample;
-        struct mali_stencil_mask_misc_packed stencil_misc;
-#endif
-};
-
-struct panfrost_zsa_state {
-        struct pipe_depth_stencil_alpha_state base;
 
-        /* Is any depth, stencil, or alpha testing enabled? */
-        bool enabled;
-
-        /* Does the depth and stencil tests always pass? This ignores write
-         * masks, we are only interested in whether pixels may be killed.
-         */
-        bool zs_always_passes;
-
-        /* Are depth or stencil writes possible? */
-        bool writes_zs;
-
-#if PAN_ARCH <= 7
-        /* Prepacked words from the RSD */
-        struct mali_multisample_misc_packed rsd_depth;
-        struct mali_stencil_mask_misc_packed rsd_stencil;
-        struct mali_stencil_packed stencil_front, stencil_back;
+/* JOBX() is used to select the job backend helpers to call from generic
+ * functions. */
+#if PAN_ARCH <= 9
+#define JOBX(__suffix) GENX(jm_##__suffix)
 #else
-        /* Depth/stencil descriptor template */
-        struct mali_depth_stencil_packed desc;
+#error "Unsupported arch"
 #endif
-};
 
 struct panfrost_sampler_state {
-        struct pipe_sampler_state base;
-        struct mali_sampler_packed hw;
+   struct pipe_sampler_state base;
+   struct mali_sampler_packed hw;
 };
 
 /* Misnomer: Sampler view corresponds to textures, not samplers */
 
 struct panfrost_sampler_view {
-        struct pipe_sampler_view base;
-        struct panfrost_pool_ref state;
-        struct mali_texture_packed bifrost_descriptor;
-        mali_ptr texture_bo;
-        uint64_t modifier;
-
-        /* Pool used to allocate the descriptor. If NULL, defaults to the global
-         * descriptor pool. Can be set for short lived descriptors, useful for
-         * shader images on Valhall.
-         */
-        struct panfrost_pool *pool;
-};
-
-struct panfrost_vertex_state {
-        unsigned num_elements;
-        struct pipe_vertex_element pipe[PIPE_MAX_ATTRIBS];
-
-#if PAN_ARCH >= 9
-        /* Packed attribute descriptor. All fields are set at CSO create time
-         * except for stride, which must be ORed in at draw time
-         */
-        struct mali_attribute_packed attributes[PIPE_MAX_ATTRIBS];
-#else
-        /* buffers corresponds to attribute buffer, element_buffers corresponds
-         * to an index in buffers for each vertex element */
-        struct pan_vertex_buffer buffers[PIPE_MAX_ATTRIBS];
-        unsigned element_buffer[PIPE_MAX_ATTRIBS];
-        unsigned nr_bufs;
-
-        unsigned formats[PIPE_MAX_ATTRIBS];
-#endif
+   struct pipe_sampler_view base;
+   struct panfrost_pool_ref state;
+   struct mali_texture_packed bifrost_descriptor;
+   mali_ptr texture_bo;
+   uint64_t modifier;
+
+   /* Pool used to allocate the descriptor. If NULL, defaults to the global
+    * descriptor pool. Can be set for short lived descriptors, useful for
+    * shader images on Valhall.
+    */
+   struct panfrost_pool *pool;
 };
 
 /* Statically assert that PIPE_* enums match the hardware enums.
  * (As long as they match, we don't need to translate them.)
  */
-static_assert((int)PIPE_FUNC_NEVER    == MALI_FUNC_NEVER,     "must match");
-static_assert((int)PIPE_FUNC_LESS     == MALI_FUNC_LESS,      "must match");
-static_assert((int)PIPE_FUNC_EQUAL    == MALI_FUNC_EQUAL,     "must match");
-static_assert((int)PIPE_FUNC_LEQUAL   == MALI_FUNC_LEQUAL,    "must match");
-static_assert((int)PIPE_FUNC_GREATER  == MALI_FUNC_GREATER,   "must match");
+static_assert((int)PIPE_FUNC_NEVER == MALI_FUNC_NEVER, "must match");
+static_assert((int)PIPE_FUNC_LESS == MALI_FUNC_LESS, "must match");
+static_assert((int)PIPE_FUNC_EQUAL == MALI_FUNC_EQUAL, "must match");
+static_assert((int)PIPE_FUNC_LEQUAL == MALI_FUNC_LEQUAL, "must match");
+static_assert((int)PIPE_FUNC_GREATER == MALI_FUNC_GREATER, "must match");
 static_assert((int)PIPE_FUNC_NOTEQUAL == MALI_FUNC_NOT_EQUAL, "must match");
-static_assert((int)PIPE_FUNC_GEQUAL   == MALI_FUNC_GEQUAL,    "must match");
-static_assert((int)PIPE_FUNC_ALWAYS   == MALI_FUNC_ALWAYS,    "must match");
+static_assert((int)PIPE_FUNC_GEQUAL == MALI_FUNC_GEQUAL, "must match");
+static_assert((int)PIPE_FUNC_ALWAYS == MALI_FUNC_ALWAYS, "must match");
 
 static inline enum mali_sample_pattern
 panfrost_sample_pattern(unsigned samples)
 {
-        switch (samples) {
-        case 1:  return MALI_SAMPLE_PATTERN_SINGLE_SAMPLED;
-        case 4:  return MALI_SAMPLE_PATTERN_ROTATED_4X_GRID;
-        case 8:  return MALI_SAMPLE_PATTERN_D3D_8X_GRID;
-        case 16: return MALI_SAMPLE_PATTERN_D3D_16X_GRID;
-        default: unreachable("Unsupported sample count");
-        }
+   switch (samples) {
+   case 1:
+      return MALI_SAMPLE_PATTERN_SINGLE_SAMPLED;
+   case 4:
+      return MALI_SAMPLE_PATTERN_ROTATED_4X_GRID;
+   case 8:
+      return MALI_SAMPLE_PATTERN_D3D_8X_GRID;
+   case 16:
+      return MALI_SAMPLE_PATTERN_D3D_16X_GRID;
+   default:
+      unreachable("Unsupported sample count");
+   }
 }
 
 static unsigned
 translate_tex_wrap(enum pipe_tex_wrap w, bool using_nearest)
 {
-        /* CLAMP is only supported on Midgard, where it is broken for nearest
-         * filtering. Use CLAMP_TO_EDGE in that case.
-         */
-
-        switch (w) {
-        case PIPE_TEX_WRAP_REPEAT: return MALI_WRAP_MODE_REPEAT;
-        case PIPE_TEX_WRAP_CLAMP_TO_EDGE: return MALI_WRAP_MODE_CLAMP_TO_EDGE;
-        case PIPE_TEX_WRAP_CLAMP_TO_BORDER: return MALI_WRAP_MODE_CLAMP_TO_BORDER;
-        case PIPE_TEX_WRAP_MIRROR_REPEAT: return MALI_WRAP_MODE_MIRRORED_REPEAT;
-        case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_EDGE: return MALI_WRAP_MODE_MIRRORED_CLAMP_TO_EDGE;
-        case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_BORDER: return MALI_WRAP_MODE_MIRRORED_CLAMP_TO_BORDER;
+   /* CLAMP is only supported on Midgard, where it is broken for nearest
+    * filtering. Use CLAMP_TO_EDGE in that case.
+    */
+
+   switch (w) {
+   case PIPE_TEX_WRAP_REPEAT:
+      return MALI_WRAP_MODE_REPEAT;
+   case PIPE_TEX_WRAP_CLAMP_TO_EDGE:
+      return MALI_WRAP_MODE_CLAMP_TO_EDGE;
+   case PIPE_TEX_WRAP_CLAMP_TO_BORDER:
+      return MALI_WRAP_MODE_CLAMP_TO_BORDER;
+   case PIPE_TEX_WRAP_MIRROR_REPEAT:
+      return MALI_WRAP_MODE_MIRRORED_REPEAT;
+   case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_EDGE:
+      return MALI_WRAP_MODE_MIRRORED_CLAMP_TO_EDGE;
+   case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_BORDER:
+      return MALI_WRAP_MODE_MIRRORED_CLAMP_TO_BORDER;
 
 #if PAN_ARCH <= 5
-        case PIPE_TEX_WRAP_CLAMP:
-                return using_nearest ? MALI_WRAP_MODE_CLAMP_TO_EDGE :
-                                       MALI_WRAP_MODE_CLAMP;
-        case PIPE_TEX_WRAP_MIRROR_CLAMP:
-                return using_nearest ? MALI_WRAP_MODE_MIRRORED_CLAMP_TO_EDGE :
-                                       MALI_WRAP_MODE_MIRRORED_CLAMP;
+   case PIPE_TEX_WRAP_CLAMP:
+      return using_nearest ? MALI_WRAP_MODE_CLAMP_TO_EDGE
+                           : MALI_WRAP_MODE_CLAMP;
+   case PIPE_TEX_WRAP_MIRROR_CLAMP:
+      return using_nearest ? MALI_WRAP_MODE_MIRRORED_CLAMP_TO_EDGE
+                           : MALI_WRAP_MODE_MIRRORED_CLAMP;
 #endif
 
-        default: unreachable("Invalid wrap");
-        }
+   default:
+      unreachable("Invalid wrap");
+   }
 }
 
 /* The hardware compares in the wrong order order, so we have to flip before
@@ -658,152 +574,152 @@ panfrost_emit_frag_shader(struct panfrost_context *ctx,
                           struct mali_renderer_state_packed *fragmeta,
                           mali_ptr *blend_shaders)
 {
-        const struct panfrost_zsa_state *zsa = ctx->depth_stencil;
-        const struct panfrost_rasterizer *rast = ctx->rasterizer;
-        struct panfrost_compiled_shader *fs =
-                ctx->prog[PIPE_SHADER_FRAGMENT];
+   const struct panfrost_zsa_state *zsa = ctx->depth_stencil;
+   const struct panfrost_rasterizer *rast = ctx->rasterizer;
+   struct panfrost_compiled_shader *fs = ctx->prog[PIPE_SHADER_FRAGMENT];
 
-        /* We need to merge several several partial renderer state descriptors,
-         * so stage to temporary storage rather than reading back write-combine
-         * memory, which will trash performance. */
-        struct mali_renderer_state_packed rsd;
-        panfrost_prepare_fs_state(ctx, blend_shaders, &rsd);
+   /* We need to merge several several partial renderer state descriptors,
+    * so stage to temporary storage rather than reading back write-combine
+    * memory, which will trash performance. */
+   struct mali_renderer_state_packed rsd;
+   panfrost_prepare_fs_state(ctx, blend_shaders, &rsd);
 
 #if PAN_ARCH == 4
-        if (ctx->pipe_framebuffer.nr_cbufs > 0 && !blend_shaders[0]) {
-                /* Word 14: SFBD Blend Equation */
-                STATIC_ASSERT(pan_size(BLEND_EQUATION) == 4);
-                rsd.opaque[14] = ctx->blend->equation[0];
-        }
+   if (ctx->pipe_framebuffer.nr_cbufs > 0 && !blend_shaders[0]) {
+      /* Word 14: SFBD Blend Equation */
+      STATIC_ASSERT(pan_size(BLEND_EQUATION) == 4);
+      rsd.opaque[14] = ctx->blend->equation[0];
+   }
 #endif
 
-        /* Merge with CSO state and upload */
-        if (panfrost_fs_required(fs, ctx->blend, &ctx->pipe_framebuffer, zsa)) {
-                struct mali_renderer_state_packed *partial_rsd =
-                        (struct mali_renderer_state_packed *)&fs->partial_rsd;
-                STATIC_ASSERT(sizeof(fs->partial_rsd) == sizeof(*partial_rsd));
-                pan_merge(rsd, *partial_rsd, RENDERER_STATE);
-        } else {
-                pan_merge_empty_fs(&rsd);
-        }
+   /* Merge with CSO state and upload */
+   if (panfrost_fs_required(fs, ctx->blend, &ctx->pipe_framebuffer, zsa)) {
+      struct mali_renderer_state_packed *partial_rsd =
+         (struct mali_renderer_state_packed *)&fs->partial_rsd;
+      STATIC_ASSERT(sizeof(fs->partial_rsd) == sizeof(*partial_rsd));
+      pan_merge(rsd, *partial_rsd, RENDERER_STATE);
+   } else {
+      pan_merge_empty_fs(&rsd);
+   }
 
-        /* Word 8, 9 Misc state */
-        rsd.opaque[8] |= zsa->rsd_depth.opaque[0]
-                       | rast->multisample.opaque[0];
+   /* Word 8, 9 Misc state */
+   rsd.opaque[8] |= zsa->rsd_depth.opaque[0] | rast->multisample.opaque[0];
 
-        rsd.opaque[9] |= zsa->rsd_stencil.opaque[0]
-                       | rast->stencil_misc.opaque[0];
+   rsd.opaque[9] |= zsa->rsd_stencil.opaque[0] | rast->stencil_misc.opaque[0];
 
-        /* Word 10, 11 Stencil Front and Back */
-        rsd.opaque[10] |= zsa->stencil_front.opaque[0];
-        rsd.opaque[11] |= zsa->stencil_back.opaque[0];
+   /* Word 10, 11 Stencil Front and Back */
+   rsd.opaque[10] |= zsa->stencil_front.opaque[0];
+   rsd.opaque[11] |= zsa->stencil_back.opaque[0];
 
-        memcpy(fragmeta, &rsd, sizeof(rsd));
+   memcpy(fragmeta, &rsd, sizeof(rsd));
 }
 
 static mali_ptr
 panfrost_emit_frag_shader_meta(struct panfrost_batch *batch)
 {
-        struct panfrost_context *ctx = batch->ctx;
-        struct panfrost_compiled_shader *ss = ctx->prog[PIPE_SHADER_FRAGMENT];
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_compiled_shader *ss = ctx->prog[PIPE_SHADER_FRAGMENT];
 
-        panfrost_batch_add_bo(batch, ss->bin.bo, PIPE_SHADER_FRAGMENT);
+   panfrost_batch_add_bo(batch, ss->bin.bo, PIPE_SHADER_FRAGMENT);
 
-        struct panfrost_ptr xfer;
+   struct panfrost_ptr xfer;
 
 #if PAN_ARCH == 4
-        xfer = pan_pool_alloc_desc(&batch->pool.base, RENDERER_STATE);
+   xfer = pan_pool_alloc_desc(&batch->pool.base, RENDERER_STATE);
 #else
-        unsigned rt_count = MAX2(ctx->pipe_framebuffer.nr_cbufs, 1);
+   unsigned rt_count = MAX2(ctx->pipe_framebuffer.nr_cbufs, 1);
 
-        xfer = pan_pool_alloc_desc_aggregate(&batch->pool.base,
-                                             PAN_DESC(RENDERER_STATE),
-                                             PAN_DESC_ARRAY(rt_count, BLEND));
+   xfer =
+      pan_pool_alloc_desc_aggregate(&batch->pool.base, PAN_DESC(RENDERER_STATE),
+                                    PAN_DESC_ARRAY(rt_count, BLEND));
 #endif
 
-        mali_ptr blend_shaders[PIPE_MAX_COLOR_BUFS] = { 0 };
-        panfrost_get_blend_shaders(batch, blend_shaders);
+   mali_ptr blend_shaders[PIPE_MAX_COLOR_BUFS] = {0};
+   panfrost_get_blend_shaders(batch, blend_shaders);
 
-        panfrost_emit_frag_shader(ctx, (struct mali_renderer_state_packed *) xfer.cpu, blend_shaders);
+   panfrost_emit_frag_shader(ctx, (struct mali_renderer_state_packed *)xfer.cpu,
+                             blend_shaders);
 
 #if PAN_ARCH >= 5
-        panfrost_emit_blend(batch, xfer.cpu + pan_size(RENDERER_STATE), blend_shaders);
+   panfrost_emit_blend(batch, xfer.cpu + pan_size(RENDERER_STATE),
+                       blend_shaders);
 #endif
 
-        return xfer.gpu;
+   return xfer.gpu;
 }
 #endif
 
 static mali_ptr
 panfrost_emit_viewport(struct panfrost_batch *batch)
 {
-        struct panfrost_context *ctx = batch->ctx;
-        const struct pipe_viewport_state *vp = &ctx->pipe_viewport;
-        const struct pipe_scissor_state *ss = &ctx->scissor;
-        const struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
-
-        /* Derive min/max from translate/scale. Note since |x| >= 0 by
-         * definition, we have that -|x| <= |x| hence translate - |scale| <=
-         * translate + |scale|, so the ordering is correct here. */
-        float vp_minx = vp->translate[0] - fabsf(vp->scale[0]);
-        float vp_maxx = vp->translate[0] + fabsf(vp->scale[0]);
-        float vp_miny = vp->translate[1] - fabsf(vp->scale[1]);
-        float vp_maxy = vp->translate[1] + fabsf(vp->scale[1]);
-        float minz = (vp->translate[2] - fabsf(vp->scale[2]));
-        float maxz = (vp->translate[2] + fabsf(vp->scale[2]));
-
-        /* Scissor to the intersection of viewport and to the scissor, clamped
-         * to the framebuffer */
-
-        unsigned minx = MIN2(batch->key.width, MAX2((int) vp_minx, 0));
-        unsigned maxx = MIN2(batch->key.width, MAX2((int) vp_maxx, 0));
-        unsigned miny = MIN2(batch->key.height, MAX2((int) vp_miny, 0));
-        unsigned maxy = MIN2(batch->key.height, MAX2((int) vp_maxy, 0));
-
-        if (ss && rast->scissor) {
-                minx = MAX2(ss->minx, minx);
-                miny = MAX2(ss->miny, miny);
-                maxx = MIN2(ss->maxx, maxx);
-                maxy = MIN2(ss->maxy, maxy);
-        }
-
-        /* Set the range to [1, 1) so max values don't wrap round */
-        if (maxx == 0 || maxy == 0)
-                maxx = maxy = minx = miny = 1;
-
-        panfrost_batch_union_scissor(batch, minx, miny, maxx, maxy);
-        batch->scissor_culls_everything = (minx >= maxx || miny >= maxy);
-
-        /* [minx, maxx) and [miny, maxy) are exclusive ranges in the hardware */
-        maxx--;
-        maxy--;
-
-        batch->minimum_z = rast->depth_clip_near ? minz : -INFINITY;
-        batch->maximum_z = rast->depth_clip_far  ? maxz : +INFINITY;
+   struct panfrost_context *ctx = batch->ctx;
+   const struct pipe_viewport_state *vp = &ctx->pipe_viewport;
+   const struct pipe_scissor_state *ss = &ctx->scissor;
+   const struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
+
+   /* Derive min/max from translate/scale. Note since |x| >= 0 by
+    * definition, we have that -|x| <= |x| hence translate - |scale| <=
+    * translate + |scale|, so the ordering is correct here. */
+   float vp_minx = vp->translate[0] - fabsf(vp->scale[0]);
+   float vp_maxx = vp->translate[0] + fabsf(vp->scale[0]);
+   float vp_miny = vp->translate[1] - fabsf(vp->scale[1]);
+   float vp_maxy = vp->translate[1] + fabsf(vp->scale[1]);
+
+   float minz, maxz;
+   util_viewport_zmin_zmax(vp, rast->clip_halfz, &minz, &maxz);
+
+   /* Scissor to the intersection of viewport and to the scissor, clamped
+    * to the framebuffer */
+
+   unsigned minx = MIN2(batch->key.width, MAX2((int)vp_minx, 0));
+   unsigned maxx = MIN2(batch->key.width, MAX2((int)vp_maxx, 0));
+   unsigned miny = MIN2(batch->key.height, MAX2((int)vp_miny, 0));
+   unsigned maxy = MIN2(batch->key.height, MAX2((int)vp_maxy, 0));
+
+   if (ss && rast->scissor) {
+      minx = MAX2(ss->minx, minx);
+      miny = MAX2(ss->miny, miny);
+      maxx = MIN2(ss->maxx, maxx);
+      maxy = MIN2(ss->maxy, maxy);
+   }
+
+   /* Set the range to [1, 1) so max values don't wrap round */
+   if (maxx == 0 || maxy == 0)
+      maxx = maxy = minx = miny = 1;
+
+   panfrost_batch_union_scissor(batch, minx, miny, maxx, maxy);
+   batch->scissor_culls_everything = (minx >= maxx || miny >= maxy);
+
+   /* [minx, maxx) and [miny, maxy) are exclusive ranges in the hardware */
+   maxx--;
+   maxy--;
+
+   batch->minimum_z = rast->depth_clip_near ? minz : -INFINITY;
+   batch->maximum_z = rast->depth_clip_far ? maxz : +INFINITY;
 
 #if PAN_ARCH <= 7
-        struct panfrost_ptr T = pan_pool_alloc_desc(&batch->pool.base, VIEWPORT);
+   struct panfrost_ptr T = pan_pool_alloc_desc(&batch->pool.base, VIEWPORT);
 
-        pan_pack(T.cpu, VIEWPORT, cfg) {
-                cfg.scissor_minimum_x = minx;
-                cfg.scissor_minimum_y = miny;
-                cfg.scissor_maximum_x = maxx;
-                cfg.scissor_maximum_y = maxy;
+   pan_pack(T.cpu, VIEWPORT, cfg) {
+      cfg.scissor_minimum_x = minx;
+      cfg.scissor_minimum_y = miny;
+      cfg.scissor_maximum_x = maxx;
+      cfg.scissor_maximum_y = maxy;
 
-                cfg.minimum_z = batch->minimum_z;
-                cfg.maximum_z = batch->maximum_z;
-        }
+      cfg.minimum_z = batch->minimum_z;
+      cfg.maximum_z = batch->maximum_z;
+   }
 
-        return T.gpu;
+   return T.gpu;
 #else
-        pan_pack(&batch->scissor, SCISSOR, cfg) {
-                cfg.scissor_minimum_x = minx;
-                cfg.scissor_minimum_y = miny;
-                cfg.scissor_maximum_x = maxx;
-                cfg.scissor_maximum_y = maxy;
-        }
-
-        return 0;
+   pan_pack(&batch->scissor, SCISSOR, cfg) {
+      cfg.scissor_minimum_x = minx;
+      cfg.scissor_minimum_y = miny;
+      cfg.scissor_maximum_x = maxx;
+      cfg.scissor_maximum_y = maxy;
+   }
+
+   return 0;
 #endif
 }
 
@@ -818,32 +734,33 @@ panfrost_emit_viewport(struct panfrost_batch *batch)
 static mali_ptr
 panfrost_emit_depth_stencil(struct panfrost_batch *batch)
 {
-        struct panfrost_context *ctx = batch->ctx;
-        const struct panfrost_zsa_state *zsa = ctx->depth_stencil;
-        struct panfrost_rasterizer *rast = ctx->rasterizer;
-        struct panfrost_compiled_shader *fs = ctx->prog[PIPE_SHADER_FRAGMENT];
-        bool back_enab = zsa->base.stencil[1].enabled;
+   struct panfrost_context *ctx = batch->ctx;
+   const struct panfrost_zsa_state *zsa = ctx->depth_stencil;
+   struct panfrost_rasterizer *rast = ctx->rasterizer;
+   struct panfrost_compiled_shader *fs = ctx->prog[PIPE_SHADER_FRAGMENT];
+   bool back_enab = zsa->base.stencil[1].enabled;
 
-        struct panfrost_ptr T = pan_pool_alloc_desc(&batch->pool.base, DEPTH_STENCIL);
-        struct mali_depth_stencil_packed dynamic;
+   struct panfrost_ptr T =
+      pan_pool_alloc_desc(&batch->pool.base, DEPTH_STENCIL);
+   struct mali_depth_stencil_packed dynamic;
 
-        pan_pack(&dynamic, DEPTH_STENCIL, cfg) {
-                cfg.front_reference_value = ctx->stencil_ref.ref_value[0];
-                cfg.back_reference_value = ctx->stencil_ref.ref_value[back_enab ? 1 : 0];
+   pan_pack(&dynamic, DEPTH_STENCIL, cfg) {
+      cfg.front_reference_value = ctx->stencil_ref.ref_value[0];
+      cfg.back_reference_value = ctx->stencil_ref.ref_value[back_enab ? 1 : 0];
 
-                cfg.stencil_from_shader = fs->info.fs.writes_stencil;
-                cfg.depth_source = pan_depth_source(&fs->info);
+      cfg.stencil_from_shader = fs->info.fs.writes_stencil;
+      cfg.depth_source = pan_depth_source(&fs->info);
 
-                cfg.depth_bias_enable = rast->base.offset_tri;
-                cfg.depth_units = rast->base.offset_units * 2.0f;
-                cfg.depth_factor = rast->base.offset_scale;
-                cfg.depth_bias_clamp = rast->base.offset_clamp;
-        }
+      cfg.depth_bias_enable = rast->base.offset_tri;
+      cfg.depth_units = rast->base.offset_units * 2.0f;
+      cfg.depth_factor = rast->base.offset_scale;
+      cfg.depth_bias_clamp = rast->base.offset_clamp;
+   }
 
-        pan_merge(dynamic, zsa->desc, DEPTH_STENCIL);
-        memcpy(T.cpu, &dynamic, pan_size(DEPTH_STENCIL));
+   pan_merge(dynamic, zsa->desc, DEPTH_STENCIL);
+   memcpy(T.cpu, &dynamic, pan_size(DEPTH_STENCIL));
 
-        return T.gpu;
+   return T.gpu;
 }
 
 /**
@@ -1444,150 +1280,121 @@ static void
 panfrost_emit_ubo(void *base, unsigned index, mali_ptr address, size_t size)
 {
 #if PAN_ARCH >= 9
-        struct mali_buffer_packed *out = base;
+   struct mali_buffer_packed *out = base;
 
-        pan_pack(out + index, BUFFER, cfg) {
-                cfg.size = size;
-                cfg.address = address;
-        }
+   pan_pack(out + index, BUFFER, cfg) {
+      cfg.size = size;
+      cfg.address = address;
+   }
 #else
-        struct mali_uniform_buffer_packed *out = base;
+   struct mali_uniform_buffer_packed *out = base;
 
-        /* Issue (57) for the ARB_uniform_buffer_object spec says that
-         * the buffer can be larger than the uniform data inside it,
-         * so clamp ubo size to what hardware supports. */
+   /* Issue (57) for the ARB_uniform_buffer_object spec says that
+    * the buffer can be larger than the uniform data inside it,
+    * so clamp ubo size to what hardware supports. */
 
-        pan_pack(out + index, UNIFORM_BUFFER, cfg) {
-                cfg.entries = MIN2(DIV_ROUND_UP(size, 16), 1 << 12);
-                cfg.pointer = address;
-        }
+   pan_pack(out + index, UNIFORM_BUFFER, cfg) {
+      cfg.entries = MIN2(DIV_ROUND_UP(size, 16), 1 << 12);
+      cfg.pointer = address;
+   }
 #endif
 }
 
 static mali_ptr
 panfrost_emit_const_buf(struct panfrost_batch *batch,
-                        enum pipe_shader_type stage,
-                        unsigned *buffer_count,
-                        mali_ptr *push_constants,
-                        unsigned *pushed_words)
+                        enum pipe_shader_type stage, unsigned *buffer_count,
+                        mali_ptr *push_constants, unsigned *pushed_words)
 {
-        struct panfrost_context *ctx = batch->ctx;
-        struct panfrost_constant_buffer *buf = &ctx->constant_buffer[stage];
-        struct panfrost_compiled_shader *ss = ctx->prog[stage];
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_constant_buffer *buf = &ctx->constant_buffer[stage];
+   struct panfrost_compiled_shader *ss = ctx->prog[stage];
 
-        if (!ss)
-                return 0;
+   if (!ss)
+      return 0;
 
-        /* Allocate room for the sysval and the uniforms */
-        size_t sys_size = sizeof(float) * 4 * ss->info.sysvals.sysval_count;
-        struct panfrost_ptr transfer =
-                pan_pool_alloc_aligned(&batch->pool.base, sys_size, 16);
+   /* Allocate room for the sysval and the uniforms */
+   size_t sys_size = sizeof(float) * 4 * ss->sysvals.sysval_count;
+   struct panfrost_ptr transfer =
+      pan_pool_alloc_aligned(&batch->pool.base, sys_size, 16);
 
-        /* Upload sysvals requested by the shader */
-        panfrost_upload_sysvals(batch, &transfer, ss, stage);
+   /* Upload sysvals requested by the shader */
+   uint8_t *sysvals = alloca(sys_size);
+   panfrost_upload_sysvals(batch, sysvals, transfer.gpu, ss, stage);
+   memcpy(transfer.cpu, sysvals, sys_size);
 
-        /* Next up, attach UBOs. UBO count includes gaps but no sysval UBO */
-        struct panfrost_compiled_shader *shader = ctx->prog[stage];
-        unsigned ubo_count = shader->info.ubo_count - (sys_size ? 1 : 0);
-        unsigned sysval_ubo = sys_size ? ubo_count : ~0;
-        struct panfrost_ptr ubos = { 0 };
+   /* Next up, attach UBOs. UBO count includes gaps but no sysval UBO */
+   struct panfrost_compiled_shader *shader = ctx->prog[stage];
+   unsigned ubo_count = shader->info.ubo_count - (sys_size ? 1 : 0);
+   unsigned sysval_ubo = sys_size ? ubo_count : ~0;
+   struct panfrost_ptr ubos = {0};
 
 #if PAN_ARCH >= 9
-        ubos = pan_pool_alloc_desc_array(&batch->pool.base,
-                                         ubo_count + 1,
-                                         BUFFER);
+   ubos = pan_pool_alloc_desc_array(&batch->pool.base, ubo_count + 1, BUFFER);
 #else
-        ubos = pan_pool_alloc_desc_array(&batch->pool.base,
-                                         ubo_count + 1,
-                                         UNIFORM_BUFFER);
+   ubos = pan_pool_alloc_desc_array(&batch->pool.base, ubo_count + 1,
+                                    UNIFORM_BUFFER);
 #endif
 
-        if (buffer_count)
-                *buffer_count = ubo_count + (sys_size ? 1 : 0);
-
-        /* Upload sysval as a final UBO */
-
-        if (sys_size)
-                panfrost_emit_ubo(ubos.cpu, ubo_count, transfer.gpu, sys_size);
-
-        /* The rest are honest-to-goodness UBOs */
-
-        u_foreach_bit(ubo, ss->info.ubo_mask & buf->enabled_mask) {
-                size_t usz = buf->cb[ubo].buffer_size;
-                mali_ptr address = 0;
-
-                if (usz > 0) {
-                        address = panfrost_map_constant_buffer_gpu(batch,
-                                        stage, buf, ubo);
-                }
-
-                panfrost_emit_ubo(ubos.cpu, ubo, address, usz);
-        }
-
-        if (pushed_words)
-                *pushed_words = ss->info.push.count;
-
-        if (ss->info.push.count == 0)
-                return ubos.gpu;
-
-        /* Copy push constants required by the shader */
-        struct panfrost_ptr push_transfer =
-                pan_pool_alloc_aligned(&batch->pool.base,
-                                       ss->info.push.count * 4, 16);
-
-        uint32_t *push_cpu = (uint32_t *) push_transfer.cpu;
-        *push_constants = push_transfer.gpu;
-
-        for (unsigned i = 0; i < ss->info.push.count; ++i) {
-                struct panfrost_ubo_word src = ss->info.push.words[i];
-
-                if (src.ubo == sysval_ubo) {
-                        unsigned sysval_idx = src.offset / 16;
-                        unsigned sysval_comp = (src.offset % 16) / 4;
-                        unsigned sysval_type = PAN_SYSVAL_TYPE(ss->info.sysvals.sysvals[sysval_idx]);
-                        mali_ptr ptr = push_transfer.gpu + (4 * i);
-
-                        switch (sysval_type) {
-                        case PAN_SYSVAL_VERTEX_INSTANCE_OFFSETS:
-                                switch (sysval_comp) {
-                                case 0:
-                                        batch->ctx->first_vertex_sysval_ptr = ptr;
-                                        break;
-                                case 1:
-                                        batch->ctx->base_vertex_sysval_ptr = ptr;
-                                        break;
-                                case 2:
-                                        batch->ctx->base_instance_sysval_ptr = ptr;
-                                        break;
-                                case 3:
-                                        /* Spurious (Midgard doesn't pack) */
-                                        break;
-                                default:
-                                        unreachable("Invalid vertex/instance offset component\n");
-                                }
-                                break;
-
-                        case PAN_SYSVAL_NUM_WORK_GROUPS:
-                                batch->num_wg_sysval[sysval_comp] = ptr;
-                                break;
-
-                        default:
-                                break;
-                        }
-                }
-                /* Map the UBO, this should be cheap. However this is reading
-                 * from write-combine memory which is _very_ slow. It might pay
-                 * off to upload sysvals to a staging buffer on the CPU on the
-                 * assumption sysvals will get pushed (TODO) */
-
-                const void *mapped_ubo = (src.ubo == sysval_ubo) ? transfer.cpu :
-                        panfrost_map_constant_buffer_cpu(ctx, buf, src.ubo);
-
-                /* TODO: Is there any benefit to combining ranges */
-                memcpy(push_cpu + i, (uint8_t *) mapped_ubo + src.offset, 4);
-        }
-
-        return ubos.gpu;
+   if (buffer_count)
+      *buffer_count = ubo_count + (sys_size ? 1 : 0);
+
+   /* Upload sysval as a final UBO */
+
+   if (sys_size)
+      panfrost_emit_ubo(ubos.cpu, ubo_count, transfer.gpu, sys_size);
+
+   /* The rest are honest-to-goodness UBOs */
+
+   u_foreach_bit(ubo, ss->info.ubo_mask & buf->enabled_mask) {
+      size_t usz = buf->cb[ubo].buffer_size;
+      mali_ptr address = 0;
+
+      if (usz > 0) {
+         address = panfrost_map_constant_buffer_gpu(batch, stage, buf, ubo);
+      }
+
+      panfrost_emit_ubo(ubos.cpu, ubo, address, usz);
+   }
+
+   if (pushed_words)
+      *pushed_words = ss->info.push.count;
+
+   if (ss->info.push.count == 0)
+      return ubos.gpu;
+
+   /* Copy push constants required by the shader */
+   struct panfrost_ptr push_transfer =
+      pan_pool_alloc_aligned(&batch->pool.base, ss->info.push.count * 4, 16);
+
+   uint32_t *push_cpu = (uint32_t *)push_transfer.cpu;
+   *push_constants = push_transfer.gpu;
+
+   for (unsigned i = 0; i < ss->info.push.count; ++i) {
+      struct panfrost_ubo_word src = ss->info.push.words[i];
+
+      if (src.ubo == sysval_ubo) {
+         unsigned sysval_idx = src.offset / 16;
+         unsigned sysval_comp = (src.offset % 16) / 4;
+         unsigned sysval_type =
+            PAN_SYSVAL_TYPE(ss->sysvals.sysvals[sysval_idx]);
+         mali_ptr ptr = push_transfer.gpu + (4 * i);
+
+         if (sysval_type == PAN_SYSVAL_NUM_WORK_GROUPS)
+            batch->num_wg_sysval[sysval_comp] = ptr;
+      }
+      /* Map the UBO, this should be cheap. For some buffers this may
+       * read from write-combine memory which is slow, though :-(
+       */
+      const void *mapped_ubo =
+         (src.ubo == sysval_ubo)
+            ? sysvals
+            : panfrost_map_constant_buffer_cpu(ctx, buf, src.ubo);
+
+      /* TODO: Is there any benefit to combining ranges */
+      memcpy(push_cpu + i, (uint8_t *)mapped_ubo + src.offset, 4);
+   }
+
+   return ubos.gpu;
 }
 
 /*
@@ -2726,208 +2491,84 @@ static void
 panfrost_initialize_surface(struct panfrost_batch *batch,
                             struct pipe_surface *surf)
 {
-        if (surf) {
-                struct panfrost_resource *rsrc = pan_resource(surf->texture);
-                BITSET_SET(rsrc->valid.data, surf->u.tex.level);
-        }
+   if (surf) {
+      struct panfrost_resource *rsrc = pan_resource(surf->texture);
+      BITSET_SET(rsrc->valid.data, surf->u.tex.level);
+   }
 }
 
 /* Generate a fragment job. This should be called once per frame. (Usually,
  * this corresponds to eglSwapBuffers or one of glFlush, glFinish)
  */
-static mali_ptr
+static void
 emit_fragment_job(struct panfrost_batch *batch, const struct pan_fb_info *pfb)
 {
-        /* Mark the affected buffers as initialized, since we're writing to it.
-         * Also, add the surfaces we're writing to to the batch */
+   /* Mark the affected buffers as initialized, since we're writing to it.
+    * Also, add the surfaces we're writing to to the batch */
 
-        struct pipe_framebuffer_state *fb = &batch->key;
+   struct pipe_framebuffer_state *fb = &batch->key;
 
-        for (unsigned i = 0; i < fb->nr_cbufs; ++i)
-                panfrost_initialize_surface(batch, fb->cbufs[i]);
+   for (unsigned i = 0; i < fb->nr_cbufs; ++i)
+      panfrost_initialize_surface(batch, fb->cbufs[i]);
 
-        panfrost_initialize_surface(batch, fb->zsbuf);
+   panfrost_initialize_surface(batch, fb->zsbuf);
 
-        /* The passed tile coords can be out of range in some cases, so we need
-         * to clamp them to the framebuffer size to avoid a TILE_RANGE_FAULT.
-         * Theoretically we also need to clamp the coordinates positive, but we
-         * avoid that edge case as all four values are unsigned. Also,
-         * theoretically we could clamp the minima, but if that has to happen
-         * the asserts would fail anyway (since the maxima would get clamped
-         * and then be smaller than the minima). An edge case of sorts occurs
-         * when no scissors are added to draw, so by default min=~0 and max=0.
-         * But that can't happen if any actual drawing occurs (beyond a
-         * wallpaper reload), so this is again irrelevant in practice. */
+   /* The passed tile coords can be out of range in some cases, so we need
+    * to clamp them to the framebuffer size to avoid a TILE_RANGE_FAULT.
+    * Theoretically we also need to clamp the coordinates positive, but we
+    * avoid that edge case as all four values are unsigned. Also,
+    * theoretically we could clamp the minima, but if that has to happen
+    * the asserts would fail anyway (since the maxima would get clamped
+    * and then be smaller than the minima). An edge case of sorts occurs
+    * when no scissors are added to draw, so by default min=~0 and max=0.
+    * But that can't happen if any actual drawing occurs (beyond a
+    * wallpaper reload), so this is again irrelevant in practice. */
 
-        batch->maxx = MIN2(batch->maxx, fb->width);
-        batch->maxy = MIN2(batch->maxy, fb->height);
+   batch->maxx = MIN2(batch->maxx, fb->width);
+   batch->maxy = MIN2(batch->maxy, fb->height);
 
-        /* Rendering region must be at least 1x1; otherwise, there is nothing
-         * to do and the whole job chain should have been discarded. */
+   /* Rendering region must be at least 1x1; otherwise, there is nothing
+    * to do and the whole job chain should have been discarded. */
 
-        assert(batch->maxx > batch->minx);
-        assert(batch->maxy > batch->miny);
-
-        struct panfrost_ptr transfer =
-                pan_pool_alloc_desc(&batch->pool.base, FRAGMENT_JOB);
-
-        GENX(pan_emit_fragment_job)(pfb, batch->framebuffer.gpu,
-                                    transfer.cpu);
-
-        return transfer.gpu;
-}
-
-#define DEFINE_CASE(c) case PIPE_PRIM_##c: return MALI_DRAW_MODE_##c;
-
-static uint8_t
-pan_draw_mode(enum pipe_prim_type mode)
-{
-        switch (mode) {
-                DEFINE_CASE(POINTS);
-                DEFINE_CASE(LINES);
-                DEFINE_CASE(LINE_LOOP);
-                DEFINE_CASE(LINE_STRIP);
-                DEFINE_CASE(TRIANGLES);
-                DEFINE_CASE(TRIANGLE_STRIP);
-                DEFINE_CASE(TRIANGLE_FAN);
-                DEFINE_CASE(QUADS);
-                DEFINE_CASE(POLYGON);
-#if PAN_ARCH <= 6
-                DEFINE_CASE(QUAD_STRIP);
-#endif
+   assert(batch->maxx > batch->minx);
+   assert(batch->maxy > batch->miny);
 
-        default:
-                unreachable("Invalid draw mode");
-        }
+   JOBX(emit_fragment_job)(batch, pfb);
 }
 
-#undef DEFINE_CASE
-
 /* Count generated primitives (when there is no geom/tess shaders) for
  * transform feedback */
 
 static void
-panfrost_statistics_record(
-                struct panfrost_context *ctx,
-                const struct pipe_draw_info *info,
-                const struct pipe_draw_start_count_bias *draw)
+panfrost_statistics_record(struct panfrost_context *ctx,
+                           const struct pipe_draw_info *info,
+                           const struct pipe_draw_start_count_bias *draw)
 {
-        if (!ctx->active_queries)
-                return;
+   if (!ctx->active_queries)
+      return;
 
-        uint32_t prims = u_prims_for_vertices(info->mode, draw->count);
-        ctx->prims_generated += prims;
+   uint32_t prims = u_prims_for_vertices(info->mode, draw->count);
+   ctx->prims_generated += prims;
 
-        if (!ctx->streamout.num_targets)
-                return;
+   if (!ctx->streamout.num_targets)
+      return;
 
-        ctx->tf_prims_generated += prims;
-        ctx->dirty |= PAN_DIRTY_SO;
+   ctx->tf_prims_generated += prims;
+   ctx->dirty |= PAN_DIRTY_SO;
 }
 
 static void
 panfrost_update_streamout_offsets(struct panfrost_context *ctx)
 {
-        unsigned count = u_stream_outputs_for_vertices(ctx->active_prim,
-                                                       ctx->vertex_count);
-
-        for (unsigned i = 0; i < ctx->streamout.num_targets; ++i) {
-                if (!ctx->streamout.targets[i])
-                        continue;
-
-                pan_so_target(ctx->streamout.targets[i])->offset += count;
-        }
-}
-
-static inline enum mali_index_type
-panfrost_translate_index_size(unsigned size)
-{
-        STATIC_ASSERT(MALI_INDEX_TYPE_NONE  == 0);
-        STATIC_ASSERT(MALI_INDEX_TYPE_UINT8  == 1);
-        STATIC_ASSERT(MALI_INDEX_TYPE_UINT16 == 2);
-
-        return (size == 4) ? MALI_INDEX_TYPE_UINT32 : size;
-}
-
-#if PAN_ARCH <= 7
-static inline void
-pan_emit_draw_descs(struct panfrost_batch *batch,
-                struct MALI_DRAW *d, enum pipe_shader_type st)
-{
-        d->offset_start = batch->ctx->offset_start;
-        d->instance_size = batch->ctx->instance_count > 1 ?
-                           batch->ctx->padded_count : 1;
-
-        d->uniform_buffers = batch->uniform_buffers[st];
-        d->push_uniforms = batch->push_uniforms[st];
-        d->textures = batch->textures[st];
-        d->samplers = batch->samplers[st];
-}
-
-static void
-panfrost_draw_emit_vertex_section(struct panfrost_batch *batch,
-                                  mali_ptr vs_vary, mali_ptr varyings,
-                                  mali_ptr attribs, mali_ptr attrib_bufs,
-                                  void *section)
-{
-        pan_pack(section, DRAW, cfg) {
-                cfg.state = batch->rsd[PIPE_SHADER_VERTEX];
-                cfg.attributes = attribs;
-                cfg.attribute_buffers = attrib_bufs;
-                cfg.varyings = vs_vary;
-                cfg.varying_buffers = vs_vary ? varyings : 0;
-                cfg.thread_storage = batch->tls.gpu;
-                pan_emit_draw_descs(batch, &cfg, PIPE_SHADER_VERTEX);
-        }
-}
-
-static void
-panfrost_draw_emit_vertex(struct panfrost_batch *batch,
-                          const struct pipe_draw_info *info,
-                          void *invocation_template,
-                          mali_ptr vs_vary, mali_ptr varyings,
-                          mali_ptr attribs, mali_ptr attrib_bufs,
-                          void *job)
-{
-        void *section =
-                pan_section_ptr(job, COMPUTE_JOB, INVOCATION);
-        memcpy(section, invocation_template, pan_size(INVOCATION));
-
-        pan_section_pack(job, COMPUTE_JOB, PARAMETERS, cfg) {
-                cfg.job_task_split = 5;
-        }
+   unsigned count =
+      u_stream_outputs_for_vertices(ctx->active_prim, ctx->vertex_count);
 
-        section = pan_section_ptr(job, COMPUTE_JOB, DRAW);
-        panfrost_draw_emit_vertex_section(batch, vs_vary, varyings,
-                                          attribs, attrib_bufs, section);
-}
-#endif
-
-static void
-panfrost_emit_primitive_size(struct panfrost_context *ctx,
-                             bool points, mali_ptr size_array,
-                             void *prim_size)
-{
-        struct panfrost_rasterizer *rast = ctx->rasterizer;
-
-        pan_pack(prim_size, PRIMITIVE_SIZE, cfg) {
-                if (panfrost_writes_point_size(ctx)) {
-                        cfg.size_array = size_array;
-                } else {
-                        cfg.constant = points ?
-                                       rast->base.point_size :
-                                       rast->base.line_width;
-                }
-        }
-}
+   for (unsigned i = 0; i < ctx->streamout.num_targets; ++i) {
+      if (!ctx->streamout.targets[i])
+         continue;
 
-static bool
-panfrost_is_implicit_prim_restart(const struct pipe_draw_info *info)
-{
-       /* As a reminder primitive_restart should always be checked before any
-          access to restart_index. */
-        return info->primitive_restart &&
-                info->restart_index == (unsigned)BITFIELD_MASK(info->index_size * 8);
+      pan_so_target(ctx->streamout.targets[i])->offset += count;
+   }
 }
 
 /* On Bifrost and older, the Renderer State Descriptor aggregates many pieces of
@@ -2941,1126 +2582,398 @@ panfrost_is_implicit_prim_restart(const struct pipe_draw_info *info)
  * specified in the draw call descriptor, but must be considered when determing
  * early-Z state which is part of the RSD.
  */
-#define FRAGMENT_RSD_DIRTY_MASK ( \
-        PAN_DIRTY_ZS | PAN_DIRTY_BLEND | PAN_DIRTY_MSAA | \
-        PAN_DIRTY_RASTERIZER | PAN_DIRTY_OQ)
+#define FRAGMENT_RSD_DIRTY_MASK                                                \
+   (PAN_DIRTY_ZS | PAN_DIRTY_BLEND | PAN_DIRTY_MSAA | PAN_DIRTY_RASTERIZER |   \
+    PAN_DIRTY_OQ)
 
 static inline void
 panfrost_update_shader_state(struct panfrost_batch *batch,
                              enum pipe_shader_type st)
 {
-        struct panfrost_context *ctx = batch->ctx;
-        struct panfrost_compiled_shader *ss = ctx->prog[st];
-
-        bool frag = (st == PIPE_SHADER_FRAGMENT);
-        unsigned dirty_3d = ctx->dirty;
-        unsigned dirty = ctx->dirty_shader[st];
-
-        if (dirty & PAN_DIRTY_STAGE_TEXTURE) {
-                batch->textures[st] =
-                        panfrost_emit_texture_descriptors(batch, st);
-        }
-
-        if (dirty & PAN_DIRTY_STAGE_SAMPLER) {
-                batch->samplers[st] =
-                        panfrost_emit_sampler_descriptors(batch, st);
-        }
-
-        /* On Bifrost and older, the fragment shader descriptor is fused
-         * together with the renderer state; the combined renderer state
-         * descriptor is emitted below. Otherwise, the shader descriptor is
-         * standalone and is emitted here.
-         */
-        if ((dirty & PAN_DIRTY_STAGE_SHADER) && !((PAN_ARCH <= 7) && frag)) {
-                batch->rsd[st] = panfrost_emit_compute_shader_meta(batch, st);
-        }
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_compiled_shader *ss = ctx->prog[st];
+
+   bool frag = (st == PIPE_SHADER_FRAGMENT);
+   unsigned dirty_3d = ctx->dirty;
+   unsigned dirty = ctx->dirty_shader[st];
+
+   if (dirty & (PAN_DIRTY_STAGE_TEXTURE | PAN_DIRTY_STAGE_SHADER)) {
+      batch->textures[st] = panfrost_emit_texture_descriptors(batch, st);
+   }
+
+   if (dirty & PAN_DIRTY_STAGE_SAMPLER) {
+      batch->samplers[st] = panfrost_emit_sampler_descriptors(batch, st);
+   }
+
+   /* On Bifrost and older, the fragment shader descriptor is fused
+    * together with the renderer state; the combined renderer state
+    * descriptor is emitted below. Otherwise, the shader descriptor is
+    * standalone and is emitted here.
+    */
+   if ((dirty & PAN_DIRTY_STAGE_SHADER) && !((PAN_ARCH <= 7) && frag)) {
+      batch->rsd[st] = panfrost_emit_compute_shader_meta(batch, st);
+   }
 
 #if PAN_ARCH >= 9
-        if (dirty & PAN_DIRTY_STAGE_IMAGE)
-                batch->images[st] = panfrost_emit_images(batch, st);
+   if (dirty & PAN_DIRTY_STAGE_IMAGE) {
+      batch->images[st] =
+         ctx->image_mask[st] ? panfrost_emit_images(batch, st) : 0;
+   }
 #endif
 
-        if ((dirty & ss->dirty_shader) || (dirty_3d & ss->dirty_3d)) {
-                batch->uniform_buffers[st] = panfrost_emit_const_buf(batch, st,
-                                NULL, &batch->push_uniforms[st], NULL);
-        }
+   if ((dirty & ss->dirty_shader) || (dirty_3d & ss->dirty_3d)) {
+      batch->uniform_buffers[st] = panfrost_emit_const_buf(
+         batch, st, &batch->nr_uniform_buffers[st], &batch->push_uniforms[st],
+         &batch->nr_push_uniforms[st]);
+   }
 
 #if PAN_ARCH <= 7
-        /* On Bifrost and older, if the fragment shader changes OR any renderer
-         * state specified with the fragment shader, the whole renderer state
-         * descriptor is dirtied and must be reemited.
-         */
-        if (frag && ((dirty & PAN_DIRTY_STAGE_SHADER) ||
-                     (dirty_3d & FRAGMENT_RSD_DIRTY_MASK))) {
-
-                batch->rsd[st] = panfrost_emit_frag_shader_meta(batch);
-        }
-
-        if (frag && (dirty & PAN_DIRTY_STAGE_IMAGE)) {
-                batch->attribs[st] = panfrost_emit_image_attribs(batch,
-                                &batch->attrib_bufs[st], st);
-        }
+   /* On Bifrost and older, if the fragment shader changes OR any renderer
+    * state specified with the fragment shader, the whole renderer state
+    * descriptor is dirtied and must be reemited.
+    */
+   if (frag && ((dirty & PAN_DIRTY_STAGE_SHADER) ||
+                (dirty_3d & FRAGMENT_RSD_DIRTY_MASK))) {
+
+      batch->rsd[st] = panfrost_emit_frag_shader_meta(batch);
+   }
+
+   /* Vertex shaders need to mix vertex data and image descriptors in the
+    * attribute array. This is taken care of in panfrost_update_state_3d().
+    */
+   if (st != PIPE_SHADER_VERTEX && (dirty & PAN_DIRTY_STAGE_IMAGE)) {
+      batch->attribs[st] =
+         panfrost_emit_image_attribs(batch, &batch->attrib_bufs[st], st);
+   }
 #endif
 }
 
 static inline void
 panfrost_update_state_3d(struct panfrost_batch *batch)
 {
-        struct panfrost_context *ctx = batch->ctx;
-        unsigned dirty = ctx->dirty;
+   struct panfrost_context *ctx = batch->ctx;
+   unsigned dirty = ctx->dirty;
 
-        if (dirty & PAN_DIRTY_TLS_SIZE)
-                panfrost_batch_adjust_stack_size(batch);
+   if (dirty & PAN_DIRTY_TLS_SIZE)
+      panfrost_batch_adjust_stack_size(batch);
 
-        if (dirty & PAN_DIRTY_BLEND)
-                panfrost_set_batch_masks_blend(batch);
+   if (dirty & PAN_DIRTY_BLEND)
+      panfrost_set_batch_masks_blend(batch);
 
-        if (dirty & PAN_DIRTY_ZS)
-                panfrost_set_batch_masks_zs(batch);
+   if (dirty & PAN_DIRTY_ZS)
+      panfrost_set_batch_masks_zs(batch);
 
 #if PAN_ARCH >= 9
-        if ((dirty & (PAN_DIRTY_ZS | PAN_DIRTY_RASTERIZER)) ||
-            (ctx->dirty_shader[PIPE_SHADER_FRAGMENT] & PAN_DIRTY_STAGE_SHADER))
-                batch->depth_stencil = panfrost_emit_depth_stencil(batch);
-
-        if (dirty & PAN_DIRTY_BLEND)
-                batch->blend = panfrost_emit_blend_valhall(batch);
-
-        if (dirty & PAN_DIRTY_VERTEX) {
-                batch->attribs[PIPE_SHADER_VERTEX] =
-                        panfrost_emit_vertex_data(batch);
-
-                batch->attrib_bufs[PIPE_SHADER_VERTEX] =
-                        panfrost_emit_vertex_buffers(batch);
-        }
-#endif
-}
+   if ((dirty & (PAN_DIRTY_ZS | PAN_DIRTY_RASTERIZER)) ||
+       (ctx->dirty_shader[PIPE_SHADER_FRAGMENT] & PAN_DIRTY_STAGE_SHADER))
+      batch->depth_stencil = panfrost_emit_depth_stencil(batch);
 
-#if PAN_ARCH >= 6
-static mali_ptr
-panfrost_batch_get_bifrost_tiler(struct panfrost_batch *batch, unsigned vertex_count)
-{
-        struct panfrost_device *dev = pan_device(batch->ctx->base.screen);
-
-        if (!vertex_count)
-                return 0;
-
-        if (batch->tiler_ctx.bifrost)
-                return batch->tiler_ctx.bifrost;
-
-        struct panfrost_ptr t =
-                pan_pool_alloc_desc(&batch->pool.base, TILER_HEAP);
-
-        GENX(pan_emit_tiler_heap)(dev, t.cpu);
+   if (dirty & PAN_DIRTY_BLEND)
+      batch->blend = panfrost_emit_blend_valhall(batch);
 
-        mali_ptr heap = t.gpu;
+   if (dirty & PAN_DIRTY_VERTEX) {
+      batch->attribs[PIPE_SHADER_VERTEX] = panfrost_emit_vertex_data(batch);
 
-        t = pan_pool_alloc_desc(&batch->pool.base, TILER_CONTEXT);
-        GENX(pan_emit_tiler_ctx)(dev, batch->key.width, batch->key.height,
-                                 util_framebuffer_get_num_samples(&batch->key),
-                                 pan_tristate_get(batch->first_provoking_vertex),
-                                 heap, t.cpu);
+      batch->attrib_bufs[PIPE_SHADER_VERTEX] =
+         panfrost_emit_vertex_buffers(batch);
+   }
+#else
+   unsigned vt_shader_dirty = ctx->dirty_shader[PIPE_SHADER_VERTEX];
 
-        batch->tiler_ctx.bifrost = t.gpu;
-        return batch->tiler_ctx.bifrost;
-}
+   /* Vertex data, vertex shader and images accessed by the vertex shader have
+    * an impact on the attributes array, we need to re-emit anytime one of these
+    * parameters changes. */
+   if ((dirty & PAN_DIRTY_VERTEX) ||
+       (vt_shader_dirty & (PAN_DIRTY_STAGE_IMAGE | PAN_DIRTY_STAGE_SHADER))) {
+      batch->attribs[PIPE_SHADER_VERTEX] = panfrost_emit_vertex_data(
+         batch, &batch->attrib_bufs[PIPE_SHADER_VERTEX]);
+   }
 #endif
+}
 
-/* Packs a primitive descriptor, mostly common between Midgard/Bifrost tiler
- * jobs and Valhall IDVS jobs
- */
 static void
-panfrost_emit_primitive(struct panfrost_context *ctx,
-                        const struct pipe_draw_info *info,
-                        const struct pipe_draw_start_count_bias *draw,
-                        mali_ptr indices, bool secondary_shader, void *out)
+panfrost_launch_xfb(struct panfrost_batch *batch,
+                    const struct pipe_draw_info *info, unsigned count)
 {
-        UNUSED struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
-
-        bool lines = (info->mode == PIPE_PRIM_LINES ||
-                      info->mode == PIPE_PRIM_LINE_LOOP ||
-                      info->mode == PIPE_PRIM_LINE_STRIP);
-
-        pan_pack(out, PRIMITIVE, cfg) {
-                cfg.draw_mode = pan_draw_mode(info->mode);
-                if (panfrost_writes_point_size(ctx))
-                        cfg.point_size_array_format = MALI_POINT_SIZE_ARRAY_FORMAT_FP16;
-
-#if PAN_ARCH <= 8
-                /* For line primitives, PRIMITIVE.first_provoking_vertex must
-                 * be set to true and the provoking vertex is selected with
-                 * DRAW.flat_shading_vertex.
-                 */
-                if (lines)
-                        cfg.first_provoking_vertex = true;
-                else
-                        cfg.first_provoking_vertex = rast->flatshade_first;
-
-                if (panfrost_is_implicit_prim_restart(info)) {
-                        cfg.primitive_restart = MALI_PRIMITIVE_RESTART_IMPLICIT;
-                } else if (info->primitive_restart) {
-                        cfg.primitive_restart = MALI_PRIMITIVE_RESTART_EXPLICIT;
-                        cfg.primitive_restart_index = info->restart_index;
-                }
-
-                cfg.job_task_split = 6;
-#else
-                struct panfrost_compiled_shader *fs =
-                        ctx->prog[PIPE_SHADER_FRAGMENT];
-
-                cfg.allow_rotating_primitives = !(lines || fs->info.bifrost.uses_flat_shading);
-                cfg.primitive_restart = info->primitive_restart;
+   struct panfrost_context *ctx = batch->ctx;
 
-                /* Non-fixed restart indices should have been lowered */
-                assert(!cfg.primitive_restart || panfrost_is_implicit_prim_restart(info));
-#endif
-
-                cfg.index_count = ctx->indirect_draw ? 1 : draw->count;
-                cfg.index_type = panfrost_translate_index_size(info->index_size);
+   /* Nothing to do */
+   if (batch->ctx->streamout.num_targets == 0)
+      return;
 
+   /* TODO: XFB with index buffers */
+   // assert(info->index_size == 0);
+   u_trim_pipe_prim(info->mode, &count);
 
-                if (PAN_ARCH >= 9) {
-                        /* Base vertex offset on Valhall is used for both
-                         * indexed and non-indexed draws, in a simple way for
-                         * either. Handle both cases.
-                         */
-                        if (cfg.index_type)
-                                cfg.base_vertex_offset = draw->index_bias;
-                        else
-                                cfg.base_vertex_offset = draw->start;
+   if (count == 0)
+      return;
 
-                        /* Indices are moved outside the primitive descriptor
-                         * on Valhall, so we don't need to set that here
-                         */
-                } else if (cfg.index_type) {
-                        cfg.base_vertex_offset = draw->index_bias - ctx->offset_start;
-
-#if PAN_ARCH <= 7
-                        cfg.indices = indices;
-#endif
-                }
-
-#if PAN_ARCH >= 6
-                cfg.secondary_shader = secondary_shader;
-#endif
-        }
-}
+   perf_debug_ctx(batch->ctx, "Emulating transform feedback");
 
-#if PAN_ARCH >= 9
-static mali_ptr
-panfrost_upload_wa_sampler(struct panfrost_batch *batch)
-{
-        struct panfrost_ptr T = pan_pool_alloc_desc(&batch->pool.base, SAMPLER);
-        pan_pack(T.cpu, SAMPLER, cfg);
-        return T.gpu;
-}
+   struct panfrost_uncompiled_shader *vs_uncompiled =
+      ctx->uncompiled[PIPE_SHADER_VERTEX];
+   struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
 
-static mali_ptr
-panfrost_emit_resources(struct panfrost_batch *batch,
-                        enum pipe_shader_type stage,
-                        mali_ptr ubos, unsigned ubo_count)
-{
-        struct panfrost_context *ctx = batch->ctx;
-        struct panfrost_ptr T;
-        unsigned nr_tables = 12;
-
-        /* Although individual resources need only 16 byte alignment, the
-         * resource table as a whole must be 64-byte aligned.
-         */
-        T = pan_pool_alloc_aligned(&batch->pool.base, nr_tables * pan_size(RESOURCE), 64);
-        memset(T.cpu, 0, nr_tables * pan_size(RESOURCE));
-
-        panfrost_make_resource_table(T, PAN_TABLE_UBO, ubos, ubo_count);
-
-        panfrost_make_resource_table(T, PAN_TABLE_TEXTURE,
-                                     batch->textures[stage],
-                                     ctx->sampler_view_count[stage]);
-
-
-        if (ctx->sampler_count[stage]) {
-                panfrost_make_resource_table(T, PAN_TABLE_SAMPLER,
-                                             batch->samplers[stage],
-                                             ctx->sampler_count[stage]);
-        } else {
-                /* We always need at least 1 sampler for txf to work */
-                panfrost_make_resource_table(T, PAN_TABLE_SAMPLER,
-                                             panfrost_upload_wa_sampler(batch),
-                                             1);
-        }
-
-        panfrost_make_resource_table(T, PAN_TABLE_IMAGE,
-                                     batch->images[stage],
-                                     util_last_bit(ctx->image_mask[stage]));
-
-        if (stage == PIPE_SHADER_VERTEX) {
-                panfrost_make_resource_table(T, PAN_TABLE_ATTRIBUTE,
-                                             batch->attribs[stage],
-                                             ctx->vertex->num_elements);
-
-                panfrost_make_resource_table(T, PAN_TABLE_ATTRIBUTE_BUFFER,
-                                             batch->attrib_bufs[stage],
-                                             util_last_bit(ctx->vb_mask));
-        }
-
-        return T.gpu | nr_tables;
-}
+   vs_uncompiled->xfb->stream_output = vs->stream_output;
 
-static void
-panfrost_emit_shader(struct panfrost_batch *batch,
-                     struct MALI_SHADER_ENVIRONMENT *cfg,
-                     enum pipe_shader_type stage,
-                     mali_ptr shader_ptr,
-                     mali_ptr thread_storage)
-{
-        unsigned fau_words = 0, ubo_count = 0;
-        mali_ptr ubos, resources;
+   mali_ptr saved_rsd = batch->rsd[PIPE_SHADER_VERTEX];
+   mali_ptr saved_ubo = batch->uniform_buffers[PIPE_SHADER_VERTEX];
+   mali_ptr saved_push = batch->push_uniforms[PIPE_SHADER_VERTEX];
+   unsigned saved_nr_push_uniforms =
+      batch->nr_push_uniforms[PIPE_SHADER_VERTEX];
 
-        ubos = panfrost_emit_const_buf(batch, stage, &ubo_count, &cfg->fau,
-                                       &fau_words);
+   ctx->uncompiled[PIPE_SHADER_VERTEX] = NULL; /* should not be read */
+   ctx->prog[PIPE_SHADER_VERTEX] = vs_uncompiled->xfb;
+   batch->rsd[PIPE_SHADER_VERTEX] =
+      panfrost_emit_compute_shader_meta(batch, PIPE_SHADER_VERTEX);
 
-        resources = panfrost_emit_resources(batch, stage, ubos, ubo_count);
+   batch->uniform_buffers[PIPE_SHADER_VERTEX] =
+      panfrost_emit_const_buf(batch, PIPE_SHADER_VERTEX, NULL,
+                              &batch->push_uniforms[PIPE_SHADER_VERTEX],
+                              &batch->nr_push_uniforms[PIPE_SHADER_VERTEX]);
 
-        cfg->thread_storage = thread_storage;
-        cfg->shader = shader_ptr;
-        cfg->resources = resources;
+   JOBX(launch_xfb)(batch, info, count);
+   batch->compute_count++;
 
-        /* Each entry of FAU is 64-bits */
-        cfg->fau_count = DIV_ROUND_UP(fau_words, 2);
+   ctx->uncompiled[PIPE_SHADER_VERTEX] = vs_uncompiled;
+   ctx->prog[PIPE_SHADER_VERTEX] = vs;
+   batch->rsd[PIPE_SHADER_VERTEX] = saved_rsd;
+   batch->uniform_buffers[PIPE_SHADER_VERTEX] = saved_ubo;
+   batch->push_uniforms[PIPE_SHADER_VERTEX] = saved_push;
+   batch->nr_push_uniforms[PIPE_SHADER_VERTEX] = saved_nr_push_uniforms;
 }
-#endif
 
-static void
-panfrost_emit_draw(void *out,
-                   struct panfrost_batch *batch,
-                   bool fs_required,
-                   enum pipe_prim_type prim,
-                   mali_ptr pos, mali_ptr fs_vary, mali_ptr varyings)
+/*
+ * Increase the vertex count on the batch using a saturating add, and hope the
+ * compiler can use the machine instruction here...
+ */
+static inline void
+panfrost_increase_vertex_count(struct panfrost_batch *batch, uint32_t increment)
 {
-        struct panfrost_context *ctx = batch->ctx;
-        struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
-        bool polygon = (prim == PIPE_PRIM_TRIANGLES);
-
-        pan_pack(out, DRAW, cfg) {
-                /*
-                 * From the Gallium documentation,
-                 * pipe_rasterizer_state::cull_face "indicates which faces of
-                 * polygons to cull". Points and lines are not considered
-                 * polygons and should be drawn even if all faces are culled.
-                 * The hardware does not take primitive type into account when
-                 * culling, so we need to do that check ourselves.
-                 */
-                cfg.cull_front_face = polygon && (rast->cull_face & PIPE_FACE_FRONT);
-                cfg.cull_back_face = polygon && (rast->cull_face & PIPE_FACE_BACK);
-                cfg.front_face_ccw = rast->front_ccw;
-
-                if (ctx->occlusion_query && ctx->active_queries) {
-                        if (ctx->occlusion_query->type == PIPE_QUERY_OCCLUSION_COUNTER)
-                                cfg.occlusion_query = MALI_OCCLUSION_MODE_COUNTER;
-                        else
-                                cfg.occlusion_query = MALI_OCCLUSION_MODE_PREDICATE;
-
-                        struct panfrost_resource *rsrc = pan_resource(ctx->occlusion_query->rsrc);
-                        cfg.occlusion = rsrc->image.data.bo->ptr.gpu;
-                        panfrost_batch_write_rsrc(ctx->batch, rsrc,
-                                              PIPE_SHADER_FRAGMENT);
-                }
-
-#if PAN_ARCH >= 9
-                struct panfrost_compiled_shader *fs =
-                        ctx->prog[PIPE_SHADER_FRAGMENT];
-
-                cfg.multisample_enable = rast->multisample;
-                cfg.sample_mask = rast->multisample ? ctx->sample_mask : 0xFFFF;
-
-                /* Use per-sample shading if required by API Also use it when a
-                 * blend shader is used with multisampling, as this is handled
-                 * by a single ST_TILE in the blend shader with the current
-                 * sample ID, requiring per-sample shading.
-                 */
-                cfg.evaluate_per_sample =
-                        (rast->multisample &&
-                         ((ctx->min_samples > 1) || ctx->valhall_has_blend_shader));
-
-                cfg.single_sampled_lines = !rast->multisample;
-
-                cfg.vertex_array.packet = true;
-
-                cfg.minimum_z = batch->minimum_z;
-                cfg.maximum_z = batch->maximum_z;
-
-                cfg.depth_stencil = batch->depth_stencil;
-
-                if (fs_required) {
-                        bool has_oq = ctx->occlusion_query && ctx->active_queries;
-
-                        struct pan_earlyzs_state earlyzs =
-                               pan_earlyzs_get(fs->earlyzs,
-                                               ctx->depth_stencil->writes_zs || has_oq,
-                                               ctx->blend->base.alpha_to_coverage,
-                                               ctx->depth_stencil->zs_always_passes);
-
-                        cfg.pixel_kill_operation = earlyzs.kill;
-                        cfg.zs_update_operation = earlyzs.update;
-
-                        cfg.allow_forward_pixel_to_kill = pan_allow_forward_pixel_to_kill(ctx, fs);
-                        cfg.allow_forward_pixel_to_be_killed = !fs->info.writes_global;
-
-                        /* Mask of render targets that may be written. A render
-                         * target may be written if the fragment shader writes
-                         * to it AND it actually exists. If the render target
-                         * doesn't actually exist, the blend descriptor will be
-                         * OFF so it may be omitted from the mask.
-                         *
-                         * Only set when there is a fragment shader, since
-                         * otherwise no colour updates are possible.
-                         */
-                        cfg.render_target_mask =
-                                (fs->info.outputs_written >> FRAG_RESULT_DATA0) &
-                                ctx->fb_rt_mask;
-
-                        /* Also use per-sample shading if required by the shader
-                         */
-                        cfg.evaluate_per_sample |= fs->info.fs.sample_shading;
-
-                        /* Unlike Bifrost, alpha-to-coverage must be included in
-                         * this identically-named flag. Confusing, isn't it?
-                         */
-                        cfg.shader_modifies_coverage = fs->info.fs.writes_coverage ||
-                                                       fs->info.fs.can_discard ||
-                                                       ctx->blend->base.alpha_to_coverage;
-
-                        /* Blend descriptors are only accessed by a BLEND
-                         * instruction on Valhall. It follows that if the
-                         * fragment shader is omitted, we may also emit the
-                         * blend descriptors.
-                         */
-                        cfg.blend = batch->blend;
-                        cfg.blend_count = MAX2(batch->key.nr_cbufs, 1);
-                        cfg.alpha_to_coverage = ctx->blend->base.alpha_to_coverage;
-
-                        cfg.overdraw_alpha0 = panfrost_overdraw_alpha(ctx, 0);
-                        cfg.overdraw_alpha1 = panfrost_overdraw_alpha(ctx, 1);
-
-                        panfrost_emit_shader(batch, &cfg.shader, PIPE_SHADER_FRAGMENT,
-                                             batch->rsd[PIPE_SHADER_FRAGMENT],
-                                             batch->tls.gpu);
-                } else {
-                        /* These operations need to be FORCE to benefit from the
-                         * depth-only pass optimizations.
-                         */
-                        cfg.pixel_kill_operation = MALI_PIXEL_KILL_FORCE_EARLY;
-                        cfg.zs_update_operation = MALI_PIXEL_KILL_FORCE_EARLY;
-
-                        /* No shader and no blend => no shader or blend
-                         * reasons to disable FPK. The only FPK-related state
-                         * not covered is alpha-to-coverage which we don't set
-                         * without blend.
-                         */
-                        cfg.allow_forward_pixel_to_kill = true;
-
-                        /* No shader => no shader side effects */
-                        cfg.allow_forward_pixel_to_be_killed = true;
-
-                        /* Alpha isn't written so these are vacuous */
-                        cfg.overdraw_alpha0 = true;
-                        cfg.overdraw_alpha1 = true;
-                }
-#else
-                cfg.position = pos;
-                cfg.state = batch->rsd[PIPE_SHADER_FRAGMENT];
-                cfg.attributes = batch->attribs[PIPE_SHADER_FRAGMENT];
-                cfg.attribute_buffers = batch->attrib_bufs[PIPE_SHADER_FRAGMENT];
-                cfg.viewport = batch->viewport;
-                cfg.varyings = fs_vary;
-                cfg.varying_buffers = fs_vary ? varyings : 0;
-                cfg.thread_storage = batch->tls.gpu;
-
-                /* For all primitives but lines DRAW.flat_shading_vertex must
-                 * be set to 0 and the provoking vertex is selected with the
-                 * PRIMITIVE.first_provoking_vertex field.
-                 */
-                if (prim == PIPE_PRIM_LINES) {
-                        /* The logic is inverted across arches. */
-                        cfg.flat_shading_vertex = rast->flatshade_first
-                                                ^ (PAN_ARCH <= 5);
-                }
-
-                pan_emit_draw_descs(batch, &cfg, PIPE_SHADER_FRAGMENT);
-#endif
-        }
-}
+   uint32_t sum = batch->tiler_ctx.vertex_count + increment;
 
-#if PAN_ARCH >= 9
-static void
-panfrost_emit_malloc_vertex(struct panfrost_batch *batch,
-                            const struct pipe_draw_info *info,
-                            const struct pipe_draw_start_count_bias *draw,
-                            mali_ptr indices, bool secondary_shader,
-                            void *job)
-{
-        struct panfrost_context *ctx = batch->ctx;
-        struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
-        struct panfrost_compiled_shader *fs = ctx->prog[PIPE_SHADER_FRAGMENT];
-
-        bool fs_required = panfrost_fs_required(fs, ctx->blend,
-                                                &ctx->pipe_framebuffer,
-                                                ctx->depth_stencil);
-
-        /* Varying shaders only feed data to the fragment shader, so if we omit
-         * the fragment shader, we should omit the varying shader too.
-         */
-        secondary_shader &= fs_required;
-
-        panfrost_emit_primitive(ctx, info, draw, 0, secondary_shader,
-                                pan_section_ptr(job, MALLOC_VERTEX_JOB, PRIMITIVE));
-
-        pan_section_pack(job, MALLOC_VERTEX_JOB, INSTANCE_COUNT, cfg) {
-                cfg.count = info->instance_count;
-        }
-
-        pan_section_pack(job, MALLOC_VERTEX_JOB, ALLOCATION, cfg) {
-                if (secondary_shader) {
-                        unsigned v = vs->info.varyings.output_count;
-                        unsigned f = fs->info.varyings.input_count;
-                        unsigned slots = MAX2(v, f);
-                        slots += util_bitcount(fs->key.fs.fixed_varying_mask);
-                        unsigned size = slots * 16;
-
-                        /* Assumes 16 byte slots. We could do better. */
-                        cfg.vertex_packet_stride = size + 16;
-                        cfg.vertex_attribute_stride = size;
-                } else {
-                        /* Hardware requirement for "no varyings" */
-                        cfg.vertex_packet_stride = 16;
-                        cfg.vertex_attribute_stride = 0;
-                }
-        }
-
-        pan_section_pack(job, MALLOC_VERTEX_JOB, TILER, cfg) {
-                cfg.address = panfrost_batch_get_bifrost_tiler(batch, ~0);
-        }
-
-        STATIC_ASSERT(sizeof(batch->scissor) == pan_size(SCISSOR));
-        memcpy(pan_section_ptr(job, MALLOC_VERTEX_JOB, SCISSOR),
-               &batch->scissor, pan_size(SCISSOR));
-
-        panfrost_emit_primitive_size(ctx, info->mode == PIPE_PRIM_POINTS, 0,
-                                     pan_section_ptr(job, MALLOC_VERTEX_JOB, PRIMITIVE_SIZE));
-
-        pan_section_pack(job, MALLOC_VERTEX_JOB, INDICES, cfg) {
-                cfg.address = indices;
-        }
-
-        panfrost_emit_draw(pan_section_ptr(job, MALLOC_VERTEX_JOB, DRAW),
-                           batch, fs_required, u_reduced_prim(info->mode), 0, 0, 0);
-
-        pan_section_pack(job, MALLOC_VERTEX_JOB, POSITION, cfg) {
-                /* IDVS/points vertex shader */
-                mali_ptr vs_ptr = batch->rsd[PIPE_SHADER_VERTEX];
-
-                /* IDVS/triangle vertex shader */
-                if (vs_ptr && info->mode != PIPE_PRIM_POINTS)
-                        vs_ptr += pan_size(SHADER_PROGRAM);
-
-                panfrost_emit_shader(batch, &cfg, PIPE_SHADER_VERTEX, vs_ptr,
-                                     batch->tls.gpu);
-        }
-
-        pan_section_pack(job, MALLOC_VERTEX_JOB, VARYING, cfg) {
-                /* If a varying shader is used, we configure it with the same
-                 * state as the position shader for backwards compatible
-                 * behaviour with Bifrost. This could be optimized.
-                 */
-                if (!secondary_shader) continue;
-
-                mali_ptr ptr = batch->rsd[PIPE_SHADER_VERTEX] +
-                                (2 * pan_size(SHADER_PROGRAM));
-
-                panfrost_emit_shader(batch, &cfg, PIPE_SHADER_VERTEX,
-                             ptr, batch->tls.gpu);
-        }
+   if (sum >= batch->tiler_ctx.vertex_count)
+      batch->tiler_ctx.vertex_count = sum;
+   else
+      batch->tiler_ctx.vertex_count = UINT32_MAX;
 }
-#endif
 
-#if PAN_ARCH <= 7
+/*
+ * If we change whether we're drawing points, or whether point sprites are
+ * enabled (specified in the rasterizer), we may need to rebind shaders
+ * accordingly. This implicitly covers the case of rebinding framebuffers,
+ * because all dirty flags are set there.
+ */
 static void
-panfrost_draw_emit_tiler(struct panfrost_batch *batch,
-                         const struct pipe_draw_info *info,
-                         const struct pipe_draw_start_count_bias *draw,
-                         void *invocation_template,
-                         mali_ptr indices, mali_ptr fs_vary, mali_ptr varyings,
-                         mali_ptr pos, mali_ptr psiz, bool secondary_shader,
-                         void *job)
+panfrost_update_point_sprite_shader(struct panfrost_context *ctx,
+                                    const struct pipe_draw_info *info)
 {
-        struct panfrost_context *ctx = batch->ctx;
-
-        void *section = pan_section_ptr(job, TILER_JOB, INVOCATION);
-        memcpy(section, invocation_template, pan_size(INVOCATION));
-
-        panfrost_emit_primitive(ctx, info, draw, indices, secondary_shader,
-                                pan_section_ptr(job, TILER_JOB, PRIMITIVE));
-
-        void *prim_size = pan_section_ptr(job, TILER_JOB, PRIMITIVE_SIZE);
-        enum pipe_prim_type prim = u_reduced_prim(info->mode);
-
-#if PAN_ARCH >= 6
-        pan_section_pack(job, TILER_JOB, TILER, cfg) {
-                cfg.address = panfrost_batch_get_bifrost_tiler(batch, ~0);
-        }
-
-        pan_section_pack(job, TILER_JOB, PADDING, cfg);
-#endif
-
-        panfrost_emit_draw(pan_section_ptr(job, TILER_JOB, DRAW),
-                           batch, true, prim, pos, fs_vary, varyings);
+   if ((ctx->dirty & PAN_DIRTY_RASTERIZER) ||
+       ((ctx->active_prim == MESA_PRIM_POINTS) ^
+        (info->mode == MESA_PRIM_POINTS))) {
 
-        panfrost_emit_primitive_size(ctx, prim == PIPE_PRIM_POINTS, psiz, prim_size);
+      ctx->active_prim = info->mode;
+      panfrost_update_shader_variant(ctx, PIPE_SHADER_FRAGMENT);
+   }
 }
-#endif
-
-static void
-panfrost_launch_xfb(struct panfrost_batch *batch,
-                    const struct pipe_draw_info *info,
-                    mali_ptr attribs, mali_ptr attrib_bufs,
-                    unsigned count)
-{
-        struct panfrost_context *ctx = batch->ctx;
-
-        struct panfrost_ptr t =
-                pan_pool_alloc_desc(&batch->pool.base, COMPUTE_JOB);
-
-        /* Nothing to do */
-        if (batch->ctx->streamout.num_targets == 0)
-                return;
-
-        /* TODO: XFB with index buffers */
-        //assert(info->index_size == 0);
-        u_trim_pipe_prim(info->mode, &count);
-
-        if (count == 0)
-                return;
-
-        perf_debug_ctx(batch->ctx, "Emulating transform feedback");
-
-        struct panfrost_uncompiled_shader *vs_uncompiled = ctx->uncompiled[PIPE_SHADER_VERTEX];
-        struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
-
-        vs_uncompiled->xfb->stream_output = vs->stream_output;
 
-        mali_ptr saved_rsd = batch->rsd[PIPE_SHADER_VERTEX];
-        mali_ptr saved_ubo = batch->uniform_buffers[PIPE_SHADER_VERTEX];
-        mali_ptr saved_push = batch->push_uniforms[PIPE_SHADER_VERTEX];
-
-        ctx->uncompiled[PIPE_SHADER_VERTEX] = NULL; /* should not be read */
-        ctx->prog[PIPE_SHADER_VERTEX] = vs_uncompiled->xfb;
-        batch->rsd[PIPE_SHADER_VERTEX] = panfrost_emit_compute_shader_meta(batch, PIPE_SHADER_VERTEX);
-
-#if PAN_ARCH >= 9
-        pan_section_pack(t.cpu, COMPUTE_JOB, PAYLOAD, cfg) {
-                cfg.workgroup_size_x = 1;
-                cfg.workgroup_size_y = 1;
-                cfg.workgroup_size_z = 1;
-
-                cfg.workgroup_count_x = count;
-                cfg.workgroup_count_y = info->instance_count;
-                cfg.workgroup_count_z = 1;
-
-                panfrost_emit_shader(batch, &cfg.compute, PIPE_SHADER_VERTEX,
-                                     batch->rsd[PIPE_SHADER_VERTEX],
-                                     batch->tls.gpu);
-
-                /* TODO: Indexing. Also, this is a legacy feature... */
-                cfg.compute.attribute_offset = batch->ctx->offset_start;
-
-                /* Transform feedback shaders do not use barriers or shared
-                 * memory, so we may merge workgroups.
-                 */
-                cfg.allow_merging_workgroups = true;
-                cfg.task_increment = 1;
-                cfg.task_axis = MALI_TASK_AXIS_Z;
-        }
-#else
-        struct mali_invocation_packed invocation;
-
-        panfrost_pack_work_groups_compute(&invocation,
-                        1, count, info->instance_count,
-                        1, 1, 1, PAN_ARCH <= 5, false);
-
-        batch->uniform_buffers[PIPE_SHADER_VERTEX] =
-                panfrost_emit_const_buf(batch, PIPE_SHADER_VERTEX, NULL,
-                                &batch->push_uniforms[PIPE_SHADER_VERTEX], NULL);
-
-        panfrost_draw_emit_vertex(batch, info, &invocation, 0, 0,
-                                  attribs, attrib_bufs, t.cpu);
-#endif
-        enum mali_job_type job_type = MALI_JOB_TYPE_COMPUTE;
-#if PAN_ARCH <= 5
-        job_type = MALI_JOB_TYPE_VERTEX;
-#endif
-        panfrost_add_job(&batch->pool.base, &batch->scoreboard, job_type,
-                         true, false, 0, 0, &t, false);
-
-        ctx->uncompiled[PIPE_SHADER_VERTEX] = vs_uncompiled;
-        ctx->prog[PIPE_SHADER_VERTEX] = vs;
-        batch->rsd[PIPE_SHADER_VERTEX] = saved_rsd;
-        batch->uniform_buffers[PIPE_SHADER_VERTEX] = saved_ubo;
-        batch->push_uniforms[PIPE_SHADER_VERTEX] = saved_push;
+static unsigned
+panfrost_draw_get_vertex_count(struct panfrost_batch *batch,
+                               const struct pipe_draw_info *info,
+                               const struct pipe_draw_start_count_bias *draw,
+                               bool idvs)
+{
+   struct panfrost_context *ctx = batch->ctx;
+   unsigned vertex_count = ctx->vertex_count;
+   unsigned min_index = 0, max_index = 0;
+
+   batch->indices = 0;
+   if (info->index_size && PAN_ARCH >= 9) {
+      batch->indices = panfrost_get_index_buffer(batch, info, draw);
+
+      /* Use index count to estimate vertex count */
+      panfrost_increase_vertex_count(batch, draw->count);
+   } else if (info->index_size) {
+      batch->indices = panfrost_get_index_buffer_bounded(
+         batch, info, draw, &min_index, &max_index);
+
+      /* Use the corresponding values */
+      vertex_count = max_index - min_index + 1;
+      ctx->offset_start = min_index + draw->index_bias;
+      panfrost_increase_vertex_count(batch, vertex_count);
+   } else {
+      ctx->offset_start = draw->start;
+      panfrost_increase_vertex_count(batch, vertex_count);
+   }
+
+   if (info->instance_count > 1) {
+      unsigned count = vertex_count;
+
+      /* Index-Driven Vertex Shading requires different instances to
+       * have different cache lines for position results. Each vertex
+       * position is 16 bytes and the Mali cache line is 64 bytes, so
+       * the instance count must be aligned to 4 vertices.
+       */
+      if (idvs)
+         count = ALIGN_POT(count, 4);
+
+      ctx->padded_count = panfrost_padded_vertex_count(count);
+   } else {
+      ctx->padded_count = vertex_count;
+   }
+
+   return vertex_count;
 }
 
 static void
 panfrost_direct_draw(struct panfrost_batch *batch,
-                     const struct pipe_draw_info *info,
-                     unsigned drawid_offset,
+                     const struct pipe_draw_info *info, unsigned drawid_offset,
                      const struct pipe_draw_start_count_bias *draw)
 {
-        if (!draw->count || !info->instance_count)
-                return;
+   if (!draw->count || !info->instance_count)
+      return;
 
-        struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_context *ctx = batch->ctx;
 
-        /* If we change whether we're drawing points, or whether point sprites
-         * are enabled (specified in the rasterizer), we may need to rebind
-         * shaders accordingly. This implicitly covers the case of rebinding
-         * framebuffers, because all dirty flags are set there.
-         */
-        if ((ctx->dirty & PAN_DIRTY_RASTERIZER) ||
-            ((ctx->active_prim == PIPE_PRIM_POINTS) ^
-             (info->mode       == PIPE_PRIM_POINTS))) {
+   panfrost_update_point_sprite_shader(ctx, info);
 
-                ctx->active_prim = info->mode;
-                panfrost_update_shader_variant(ctx, PIPE_SHADER_FRAGMENT);
-        }
+   /* Take into account a negative bias */
+   ctx->vertex_count =
+      draw->count + (info->index_size ? abs(draw->index_bias) : 0);
+   ctx->instance_count = info->instance_count;
+   ctx->base_vertex = info->index_size ? draw->index_bias : 0;
+   ctx->base_instance = info->start_instance;
+   ctx->active_prim = info->mode;
+   ctx->drawid = drawid_offset;
 
-        /* Take into account a negative bias */
-        ctx->indirect_draw = false;
-        ctx->vertex_count = draw->count + (info->index_size ? abs(draw->index_bias) : 0);
-        ctx->instance_count = info->instance_count;
-        ctx->base_vertex = info->index_size ? draw->index_bias : 0;
-        ctx->base_instance = info->start_instance;
-        ctx->active_prim = info->mode;
-        ctx->drawid = drawid_offset;
+   struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
+   bool idvs = vs->info.vs.idvs;
 
-        struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
+   UNUSED unsigned vertex_count =
+      panfrost_draw_get_vertex_count(batch, info, draw, idvs);
 
-        bool idvs = vs->info.vs.idvs;
-        bool secondary_shader = vs->info.vs.secondary_enable;
+   panfrost_statistics_record(ctx, info, draw);
 
-        UNUSED struct panfrost_ptr tiler, vertex;
+   panfrost_update_state_3d(batch);
+   panfrost_update_shader_state(batch, PIPE_SHADER_VERTEX);
+   panfrost_update_shader_state(batch, PIPE_SHADER_FRAGMENT);
+   panfrost_clean_state_3d(ctx);
 
-        if (idvs) {
-#if PAN_ARCH >= 9
-                tiler = pan_pool_alloc_desc(&batch->pool.base, MALLOC_VERTEX_JOB);
-#elif PAN_ARCH >= 6
-                tiler = pan_pool_alloc_desc(&batch->pool.base, INDEXED_VERTEX_JOB);
-#else
-                unreachable("IDVS is unsupported on Midgard");
-#endif
-        } else {
-                vertex = pan_pool_alloc_desc(&batch->pool.base, COMPUTE_JOB);
-                tiler = pan_pool_alloc_desc(&batch->pool.base, TILER_JOB);
-        }
-
-        unsigned vertex_count = ctx->vertex_count;
-
-        unsigned min_index = 0, max_index = 0;
-        mali_ptr indices = 0;
-
-        if (info->index_size && PAN_ARCH >= 9) {
-                indices = panfrost_get_index_buffer(batch, info, draw);
-        } else if (info->index_size) {
-                indices = panfrost_get_index_buffer_bounded(batch, info, draw,
-                                                            &min_index,
-                                                            &max_index);
-
-                /* Use the corresponding values */
-                vertex_count = max_index - min_index + 1;
-                ctx->offset_start = min_index + draw->index_bias;
-        } else {
-                ctx->offset_start = draw->start;
-        }
-
-        if (info->instance_count > 1) {
-                unsigned count = vertex_count;
-
-                /* Index-Driven Vertex Shading requires different instances to
-                 * have different cache lines for position results. Each vertex
-                 * position is 16 bytes and the Mali cache line is 64 bytes, so
-                 * the instance count must be aligned to 4 vertices.
-                 */
-                if (idvs)
-                        count = ALIGN_POT(count, 4);
-
-                ctx->padded_count = panfrost_padded_vertex_count(count);
-        } else
-                ctx->padded_count = vertex_count;
-
-        panfrost_statistics_record(ctx, info, draw);
+   if (ctx->uncompiled[PIPE_SHADER_VERTEX]->xfb) {
+      panfrost_launch_xfb(batch, info, draw->count);
+   }
 
-#if PAN_ARCH <= 7
-        struct mali_invocation_packed invocation;
-        if (info->instance_count > 1) {
-                panfrost_pack_work_groups_compute(&invocation,
-                                                  1, vertex_count, info->instance_count,
-                                                  1, 1, 1, true, false);
-        } else {
-                pan_pack(&invocation, INVOCATION, cfg) {
-                        cfg.invocations = MALI_POSITIVE(vertex_count);
-                        cfg.size_y_shift = 0;
-                        cfg.size_z_shift = 0;
-                        cfg.workgroups_x_shift = 0;
-                        cfg.workgroups_y_shift = 0;
-                        cfg.workgroups_z_shift = 32;
-                        cfg.thread_group_split = MALI_SPLIT_MIN_EFFICIENT;
-                }
-        }
-
-        /* Emit all sort of descriptors. */
-        mali_ptr varyings = 0, vs_vary = 0, fs_vary = 0, pos = 0, psiz = 0;
-
-        panfrost_emit_varying_descriptor(batch,
-                                         ctx->padded_count *
-                                         ctx->instance_count,
-                                         &vs_vary, &fs_vary, &varyings,
-                                         NULL, &pos, &psiz,
-                                         info->mode == PIPE_PRIM_POINTS);
-
-        mali_ptr attribs, attrib_bufs;
-        attribs = panfrost_emit_vertex_data(batch, &attrib_bufs);
-#endif
+   /* Increment transform feedback offsets */
+   panfrost_update_streamout_offsets(ctx);
 
-        panfrost_update_state_3d(batch);
-        panfrost_update_shader_state(batch, PIPE_SHADER_VERTEX);
-        panfrost_update_shader_state(batch, PIPE_SHADER_FRAGMENT);
-        panfrost_clean_state_3d(ctx);
+   /* Any side effects must be handled by the XFB shader, so we only need
+    * to run vertex shaders if we need rasterization.
+    */
+   if (panfrost_batch_skip_rasterization(batch))
+      return;
 
-        if (ctx->uncompiled[PIPE_SHADER_VERTEX]->xfb) {
-#if PAN_ARCH >= 9
-                mali_ptr attribs = 0, attrib_bufs = 0;
+#if PAN_ARCH <= 7
+   /* Emit all sort of descriptors. */
+   panfrost_emit_varying_descriptor(batch,
+                                    ctx->padded_count * ctx->instance_count,
+                                    info->mode == MESA_PRIM_POINTS);
 #endif
-                panfrost_launch_xfb(batch, info, attribs, attrib_bufs, draw->count);
-        }
-
-        /* Increment transform feedback offsets */
-        panfrost_update_streamout_offsets(ctx);
-
-        /* Any side effects must be handled by the XFB shader, so we only need
-         * to run vertex shaders if we need rasterization.
-         */
-        if (panfrost_batch_skip_rasterization(batch))
-                return;
-
-#if PAN_ARCH >= 9
-        assert(idvs && "Memory allocated IDVS required on Valhall");
-
-        panfrost_emit_malloc_vertex(batch, info, draw, indices,
-                                    secondary_shader, tiler.cpu);
 
-        panfrost_add_job(&batch->pool.base, &batch->scoreboard,
-                         MALI_JOB_TYPE_MALLOC_VERTEX, false, false, 0,
-                         0, &tiler, false);
-#else
-        /* Fire off the draw itself */
-        panfrost_draw_emit_tiler(batch, info, draw, &invocation, indices,
-                                 fs_vary, varyings, pos, psiz, secondary_shader,
-                                 tiler.cpu);
-        if (idvs) {
-#if PAN_ARCH >= 6
-                panfrost_draw_emit_vertex_section(batch,
-                                  vs_vary, varyings,
-                                  attribs, attrib_bufs,
-                                  pan_section_ptr(tiler.cpu, INDEXED_VERTEX_JOB, VERTEX_DRAW));
-
-                panfrost_add_job(&batch->pool.base, &batch->scoreboard,
-                                 MALI_JOB_TYPE_INDEXED_VERTEX, false, false,
-                                 0, 0, &tiler, false);
-#endif
-        } else {
-                panfrost_draw_emit_vertex(batch, info, &invocation,
-                                          vs_vary, varyings, attribs, attrib_bufs, vertex.cpu);
-                panfrost_emit_vertex_tiler_jobs(batch, &vertex, &tiler);
-        }
-#endif
+   JOBX(launch_draw)(batch, info, drawid_offset, draw, vertex_count);
+   batch->draw_count++;
 }
 
-#if PAN_GPU_INDIRECTS
-static void
-panfrost_indirect_draw(struct panfrost_batch *batch,
-                       const struct pipe_draw_info *info,
-                       unsigned drawid_offset,
-                       const struct pipe_draw_indirect_info *indirect,
-                       const struct pipe_draw_start_count_bias *draw)
+static bool
+panfrost_compatible_batch_state(struct panfrost_batch *batch, bool points)
 {
-        /* Indirect draw count and multi-draw not supported. */
-        assert(indirect->draw_count == 1 && !indirect->indirect_draw_count);
-
-        struct panfrost_context *ctx = batch->ctx;
-        struct panfrost_device *dev = pan_device(ctx->base.screen);
-
-        perf_debug(dev, "Emulating indirect draw on the GPU");
-
-        /* TODO: update statistics (see panfrost_statistics_record()) */
-        /* TODO: Increment transform feedback offsets */
-        assert(ctx->streamout.num_targets == 0);
-
-        ctx->active_prim = info->mode;
-        ctx->drawid = drawid_offset;
-        ctx->indirect_draw = true;
-
-        struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
+   /* Only applies on Valhall */
+   if (PAN_ARCH < 9)
+      return true;
 
-        bool idvs = vs->info.vs.idvs;
-        bool secondary_shader = vs->info.vs.secondary_enable;
+   struct panfrost_context *ctx = batch->ctx;
+   struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
 
-        struct panfrost_ptr tiler = { 0 }, vertex = { 0 };
-
-        if (idvs) {
-#if PAN_ARCH >= 6
-                tiler = pan_pool_alloc_desc(&batch->pool.base, INDEXED_VERTEX_JOB);
-#else
-                unreachable("IDVS is unsupported on Midgard");
-#endif
-        } else {
-                vertex = pan_pool_alloc_desc(&batch->pool.base, COMPUTE_JOB);
-                tiler = pan_pool_alloc_desc(&batch->pool.base, TILER_JOB);
-        }
-
-        struct panfrost_bo *index_buf = NULL;
-
-        if (info->index_size) {
-                assert(!info->has_user_indices);
-                struct panfrost_resource *rsrc = pan_resource(info->index.resource);
-                index_buf = rsrc->image.data.bo;
-                panfrost_batch_read_rsrc(batch, rsrc, PIPE_SHADER_VERTEX);
-        }
-
-        mali_ptr varyings = 0, vs_vary = 0, fs_vary = 0, pos = 0, psiz = 0;
-        unsigned varying_buf_count;
-
-        /* We want to create templates, set all count fields to 0 to reflect
-         * that.
-         */
-        ctx->instance_count = ctx->vertex_count = ctx->padded_count = 0;
-        ctx->offset_start = 0;
-
-        /* Set the {first,base}_vertex sysvals to NULL. Will be updated if the
-         * vertex shader uses gl_VertexID or gl_BaseVertex.
-         */
-        ctx->first_vertex_sysval_ptr = 0;
-        ctx->base_vertex_sysval_ptr = 0;
-        ctx->base_instance_sysval_ptr = 0;
-
-        panfrost_update_state_3d(batch);
-        panfrost_update_shader_state(batch, PIPE_SHADER_VERTEX);
-        panfrost_update_shader_state(batch, PIPE_SHADER_FRAGMENT);
-        panfrost_clean_state_3d(ctx);
-
-        bool point_coord_replace = (info->mode == PIPE_PRIM_POINTS);
-
-        panfrost_emit_varying_descriptor(batch, 0,
-                                         &vs_vary, &fs_vary, &varyings,
-                                         &varying_buf_count, &pos, &psiz,
-                                         point_coord_replace);
-
-        mali_ptr attribs, attrib_bufs;
-        attribs = panfrost_emit_vertex_data(batch, &attrib_bufs);
-
-        /* Zero-ed invocation, the compute job will update it. */
-        static struct mali_invocation_packed invocation;
-
-        /* Fire off the draw itself */
-        panfrost_draw_emit_tiler(batch, info, draw, &invocation,
-                                 index_buf ? index_buf->ptr.gpu : 0,
-                                 fs_vary, varyings, pos, psiz, secondary_shader,
-                                 tiler.cpu);
-        if (idvs) {
-#if PAN_ARCH >= 6
-                panfrost_draw_emit_vertex_section(batch,
-                                  vs_vary, varyings,
-                                  attribs, attrib_bufs,
-                                  pan_section_ptr(tiler.cpu, INDEXED_VERTEX_JOB, VERTEX_DRAW));
-#endif
-        } else {
-                panfrost_draw_emit_vertex(batch, info, &invocation,
-                                          vs_vary, varyings, attribs, attrib_bufs, vertex.cpu);
-        }
-
-        /* Add the varying heap BO to the batch if we're allocating varyings. */
-        if (varyings) {
-                panfrost_batch_add_bo(batch,
-                                      dev->indirect_draw_shaders.varying_heap,
-                                      PIPE_SHADER_VERTEX);
-        }
-
-        assert(indirect->buffer);
-
-        struct panfrost_resource *draw_buf = pan_resource(indirect->buffer);
-
-        /* Don't count images: those attributes don't need to be patched. */
-        unsigned attrib_count =
-                vs->info.attribute_count -
-                util_bitcount(ctx->image_mask[PIPE_SHADER_VERTEX]);
-
-        panfrost_batch_read_rsrc(batch, draw_buf, PIPE_SHADER_VERTEX);
-
-        struct pan_indirect_draw_info draw_info = {
-                .last_indirect_draw = batch->indirect_draw_job_id,
-                .draw_buf = draw_buf->image.data.bo->ptr.gpu + indirect->offset,
-                .index_buf = index_buf ? index_buf->ptr.gpu : 0,
-                .first_vertex_sysval = ctx->first_vertex_sysval_ptr,
-                .base_vertex_sysval = ctx->base_vertex_sysval_ptr,
-                .base_instance_sysval = ctx->base_instance_sysval_ptr,
-                .vertex_job = vertex.gpu,
-                .tiler_job = tiler.gpu,
-                .attrib_bufs = attrib_bufs,
-                .attribs = attribs,
-                .attrib_count = attrib_count,
-                .varying_bufs = varyings,
-                .index_size = info->index_size,
-        };
-
-        if (panfrost_writes_point_size(ctx))
-                draw_info.flags |= PAN_INDIRECT_DRAW_UPDATE_PRIM_SIZE;
-
-        if (vs->info.vs.writes_point_size)
-                draw_info.flags |= PAN_INDIRECT_DRAW_HAS_PSIZ;
-
-        if (idvs)
-                draw_info.flags |= PAN_INDIRECT_DRAW_IDVS;
-
-        if (info->primitive_restart) {
-                draw_info.restart_index = info->restart_index;
-                draw_info.flags |= PAN_INDIRECT_DRAW_PRIMITIVE_RESTART;
-        }
-
-        batch->indirect_draw_job_id =
-                GENX(panfrost_emit_indirect_draw)(&batch->pool.base,
-                                                  &batch->scoreboard,
-                                                  &draw_info,
-                                                  &batch->indirect_draw_ctx);
-
-        if (idvs) {
-                panfrost_add_job(&batch->pool.base, &batch->scoreboard,
-                                 MALI_JOB_TYPE_INDEXED_VERTEX, false, false,
-                                 0, 0, &tiler, false);
-        } else {
-                panfrost_emit_vertex_tiler_jobs(batch, &vertex, &tiler);
-        }
-}
-#endif
+   bool coord = (rast->sprite_coord_mode == PIPE_SPRITE_COORD_LOWER_LEFT);
+   bool first = rast->flatshade_first;
 
-static bool
-panfrost_compatible_batch_state(struct panfrost_batch *batch,
-                                bool points)
-{
-        /* Only applies on Valhall */
-        if (PAN_ARCH < 9)
-                return true;
-
-        struct panfrost_context *ctx = batch->ctx;
-        struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
-
-        bool coord = (rast->sprite_coord_mode == PIPE_SPRITE_COORD_LOWER_LEFT);
-        bool first = rast->flatshade_first;
-
-        /* gl_PointCoord orientation only matters when drawing points, but
-         * provoking vertex doesn't matter for points.
-         */
-        if (points)
-                return pan_tristate_set(&batch->sprite_coord_origin, coord);
-        else
-                return pan_tristate_set(&batch->first_provoking_vertex, first);
+   /* gl_PointCoord orientation only matters when drawing points, but
+    * provoking vertex doesn't matter for points.
+    */
+   if (points)
+      return pan_tristate_set(&batch->sprite_coord_origin, coord);
+   else
+      return pan_tristate_set(&batch->first_provoking_vertex, first);
 }
 
 static void
-panfrost_draw_vbo(struct pipe_context *pipe,
-                  const struct pipe_draw_info *info,
+panfrost_draw_vbo(struct pipe_context *pipe, const struct pipe_draw_info *info,
                   unsigned drawid_offset,
                   const struct pipe_draw_indirect_info *indirect,
                   const struct pipe_draw_start_count_bias *draws,
                   unsigned num_draws)
 {
-        struct panfrost_context *ctx = pan_context(pipe);
-        struct panfrost_device *dev = pan_device(pipe->screen);
-
-        if (!panfrost_render_condition_check(ctx))
-                return;
+   struct panfrost_context *ctx = pan_context(pipe);
+   struct panfrost_device *dev = pan_device(pipe->screen);
 
-        ctx->draw_calls++;
+   if (!panfrost_render_condition_check(ctx))
+      return;
 
-        /* Emulate indirect draws unless we're using the experimental path */
-        if ((!(dev->debug & PAN_DBG_INDIRECT) || !PAN_GPU_INDIRECTS) && indirect && indirect->buffer) {
-                assert(num_draws == 1);
-                util_draw_indirect(pipe, info, indirect);
-                perf_debug(dev, "Emulating indirect draw on the CPU");
-                return;
-        }
+   ctx->draw_calls++;
 
-        /* Do some common setup */
-        struct panfrost_batch *batch = panfrost_get_batch_for_fbo(ctx);
+   /* Emulate indirect draws on JM */
+   if (indirect && indirect->buffer) {
+      assert(num_draws == 1);
+      util_draw_indirect(pipe, info, indirect);
+      perf_debug(dev, "Emulating indirect draw on the CPU");
+      return;
+   }
 
-        /* Don't add too many jobs to a single batch. Hardware has a hard limit
-         * of 65536 jobs, but we choose a smaller soft limit (arbitrary) to
-         * avoid the risk of timeouts. This might not be a good idea. */
-        if (unlikely(batch->scoreboard.job_index > 10000))
-                batch = panfrost_get_fresh_batch_for_fbo(ctx, "Too many draws");
+   /* Do some common setup */
+   struct panfrost_batch *batch = panfrost_get_batch_for_fbo(ctx);
 
-        bool points = (info->mode == PIPE_PRIM_POINTS);
+   /* Don't add too many jobs to a single batch. Job manager hardware has a
+    * hard limit of 65536 jobs per job chain. Given a draw issues a maximum
+    * of 3 jobs (a vertex, a tiler and a compute job is XFB is enabled), we
+    * could use 65536 / 3 as a limit, but we choose a smaller soft limit
+    * (arbitrary) to avoid the risk of timeouts. This might not be a good
+    * idea. */
+   if (unlikely(batch->draw_count > 10000))
+      batch = panfrost_get_fresh_batch_for_fbo(ctx, "Too many draws");
 
-        if (unlikely(!panfrost_compatible_batch_state(batch, points))) {
-                batch = panfrost_get_fresh_batch_for_fbo(ctx, "State change");
+   bool points = (info->mode == MESA_PRIM_POINTS);
 
-                ASSERTED bool succ = panfrost_compatible_batch_state(batch, points);
-                assert(succ && "must be able to set state for a fresh batch");
-        }
+   if (unlikely(!panfrost_compatible_batch_state(batch, points))) {
+      batch = panfrost_get_fresh_batch_for_fbo(ctx, "State change");
 
-        /* panfrost_batch_skip_rasterization reads
-         * batch->scissor_culls_everything, which is set by
-         * panfrost_emit_viewport, so call that first.
-         */
-        if (ctx->dirty & (PAN_DIRTY_VIEWPORT | PAN_DIRTY_SCISSOR))
-                batch->viewport = panfrost_emit_viewport(batch);
+      ASSERTED bool succ = panfrost_compatible_batch_state(batch, points);
+      assert(succ && "must be able to set state for a fresh batch");
+   }
 
-        /* Mark everything dirty when debugging */
-        if (unlikely(dev->debug & PAN_DBG_DIRTY))
-                panfrost_dirty_state_all(ctx);
+   /* panfrost_batch_skip_rasterization reads
+    * batch->scissor_culls_everything, which is set by
+    * panfrost_emit_viewport, so call that first.
+    */
+   if (ctx->dirty & (PAN_DIRTY_VIEWPORT | PAN_DIRTY_SCISSOR))
+      batch->viewport = panfrost_emit_viewport(batch);
 
-        /* Conservatively assume draw parameters always change */
-        ctx->dirty |= PAN_DIRTY_PARAMS | PAN_DIRTY_DRAWID;
-
-        if (indirect) {
-                assert(num_draws == 1);
-                assert(PAN_GPU_INDIRECTS);
-
-#if PAN_GPU_INDIRECTS
-                if (indirect->count_from_stream_output) {
-                        struct pipe_draw_start_count_bias tmp_draw = *draws;
-                        struct panfrost_streamout_target *so =
-                                pan_so_target(indirect->count_from_stream_output);
-
-                        tmp_draw.start = 0;
-                        tmp_draw.count = so->offset;
-                        tmp_draw.index_bias = 0;
-                        panfrost_direct_draw(batch, info, drawid_offset, &tmp_draw);
-                        return;
-                }
-
-                panfrost_indirect_draw(batch, info, drawid_offset, indirect, &draws[0]);
-                return;
-#endif
-        }
+   /* Mark everything dirty when debugging */
+   if (unlikely(dev->debug & PAN_DBG_DIRTY))
+      panfrost_dirty_state_all(ctx);
 
-        struct pipe_draw_info tmp_info = *info;
-        unsigned drawid = drawid_offset;
+   /* Conservatively assume draw parameters always change */
+   ctx->dirty |= PAN_DIRTY_PARAMS | PAN_DIRTY_DRAWID;
 
-        for (unsigned i = 0; i < num_draws; i++) {
-                panfrost_direct_draw(batch, &tmp_info, drawid, &draws[i]);
+   struct pipe_draw_info tmp_info = *info;
+   unsigned drawid = drawid_offset;
 
-                if (tmp_info.increment_draw_id) {
-                        ctx->dirty |= PAN_DIRTY_DRAWID;
-                        drawid++;
-                }
-        }
+   for (unsigned i = 0; i < num_draws; i++) {
+      panfrost_direct_draw(batch, &tmp_info, drawid, &draws[i]);
 
+      if (tmp_info.increment_draw_id) {
+         ctx->dirty |= PAN_DIRTY_DRAWID;
+         drawid++;
+      }
+   }
 }
 
 /* Launch grid is the compute equivalent of draw_vbo, so in this routine, we
@@ -4068,261 +2981,282 @@ panfrost_draw_vbo(struct pipe_context *pipe,
  */
 
 static void
-panfrost_launch_grid(struct pipe_context *pipe,
-                const struct pipe_grid_info *info)
+panfrost_launch_grid_on_batch(struct pipe_context *pipe,
+                              struct panfrost_batch *batch,
+                              const struct pipe_grid_info *info)
 {
-        struct panfrost_context *ctx = pan_context(pipe);
+   struct panfrost_context *ctx = pan_context(pipe);
+
+   if (info->indirect && !PAN_GPU_INDIRECTS) {
+      struct pipe_transfer *transfer;
+      uint32_t *params =
+         pipe_buffer_map_range(pipe, info->indirect, info->indirect_offset,
+                               3 * sizeof(uint32_t), PIPE_MAP_READ, &transfer);
+
+      struct pipe_grid_info direct = *info;
+      direct.indirect = NULL;
+      direct.grid[0] = params[0];
+      direct.grid[1] = params[1];
+      direct.grid[2] = params[2];
+      pipe_buffer_unmap(pipe, transfer);
 
-        /* XXX - shouldn't be necessary with working memory barriers. Affected
-         * test: KHR-GLES31.core.compute_shader.pipeline-post-xfb */
-        panfrost_flush_all_batches(ctx, "Launch grid pre-barrier");
+      if (params[0] && params[1] && params[2])
+         panfrost_launch_grid_on_batch(pipe, batch, &direct);
 
-        struct panfrost_batch *batch = panfrost_get_batch_for_fbo(ctx);
+      return;
+   }
 
-        if (info->indirect && !PAN_GPU_INDIRECTS) {
-                struct pipe_transfer *transfer;
-                uint32_t *params = pipe_buffer_map_range(pipe, info->indirect,
-                                info->indirect_offset,
-                                3 * sizeof(uint32_t),
-                                PIPE_MAP_READ,
-                                &transfer);
+   ctx->compute_grid = info;
 
-                struct pipe_grid_info direct = *info;
-                direct.indirect = NULL;
-                direct.grid[0] = params[0];
-                direct.grid[1] = params[1];
-                direct.grid[2] = params[2];
-                pipe_buffer_unmap(pipe, transfer);
+   /* Conservatively assume workgroup size changes every launch */
+   ctx->dirty |= PAN_DIRTY_PARAMS;
 
-                if (params[0] && params[1] && params[2])
-                        panfrost_launch_grid(pipe, &direct);
+   panfrost_update_shader_state(batch, PIPE_SHADER_COMPUTE);
 
-                return;
-        }
+   /* We want our compute thread descriptor to be per job.
+    * Save the global one, and restore it when we're done emitting
+    * the job.
+    */
+   mali_ptr saved_tls = batch->tls.gpu;
+   batch->tls.gpu = panfrost_emit_shared_memory(batch, info);
 
-        ctx->compute_grid = info;
+   JOBX(launch_grid)(batch, info);
+   batch->compute_count++;
+   batch->tls.gpu = saved_tls;
+}
+
+static void
+panfrost_launch_grid(struct pipe_context *pipe,
+                     const struct pipe_grid_info *info)
+{
+   struct panfrost_context *ctx = pan_context(pipe);
 
-        struct panfrost_ptr t =
-                pan_pool_alloc_desc(&batch->pool.base, COMPUTE_JOB);
+   /* XXX - shouldn't be necessary with working memory barriers. Affected
+    * test: KHR-GLES31.core.compute_shader.pipeline-post-xfb */
+   panfrost_flush_all_batches(ctx, "Launch grid pre-barrier");
 
-        /* Invoke according to the grid info */
+   struct panfrost_batch *batch = panfrost_get_batch_for_fbo(ctx);
+   panfrost_launch_grid_on_batch(pipe, batch, info);
 
-        unsigned num_wg[3] = { info->grid[0], info->grid[1], info->grid[2] };
+   panfrost_flush_all_batches(ctx, "Launch grid post-barrier");
+}
 
-        if (info->indirect)
-                num_wg[0] = num_wg[1] = num_wg[2] = 1;
+#define AFBC_BLOCK_ALIGN 16
 
-        /* Conservatively assume workgroup size changes every launch */
-        ctx->dirty |= PAN_DIRTY_PARAMS;
+static void
+panfrost_launch_afbc_shader(struct panfrost_batch *batch, void *cso,
+                            struct pipe_constant_buffer *cbuf,
+                            unsigned nr_blocks)
+{
+   struct pipe_context *pctx = &batch->ctx->base;
+   void *saved_cso = NULL;
+   struct pipe_constant_buffer saved_const = {};
+   struct pipe_grid_info grid = {
+      .block[0] = 1,
+      .block[1] = 1,
+      .block[2] = 1,
+      .grid[0] = nr_blocks,
+      .grid[1] = 1,
+      .grid[2] = 1,
+   };
+
+   struct panfrost_constant_buffer *pbuf =
+      &batch->ctx->constant_buffer[PIPE_SHADER_COMPUTE];
+   saved_cso = batch->ctx->uncompiled[PIPE_SHADER_COMPUTE];
+   util_copy_constant_buffer(&pbuf->cb[0], &saved_const, true);
+
+   pctx->bind_compute_state(pctx, cso);
+   pctx->set_constant_buffer(pctx, PIPE_SHADER_COMPUTE, 0, false, cbuf);
+
+   panfrost_launch_grid_on_batch(pctx, batch, &grid);
+
+   pctx->bind_compute_state(pctx, saved_cso);
+   pctx->set_constant_buffer(pctx, PIPE_SHADER_COMPUTE, 0, true, &saved_const);
+}
+
+#define LAUNCH_AFBC_SHADER(name, batch, rsrc, consts, nr_blocks)               \
+   struct pan_afbc_shader_data *shaders =                                      \
+      panfrost_afbc_get_shaders(batch->ctx, rsrc, AFBC_BLOCK_ALIGN);           \
+   struct pipe_constant_buffer constant_buffer = {                             \
+      .buffer_size = sizeof(consts),                                           \
+      .user_buffer = &consts};                                                 \
+   panfrost_launch_afbc_shader(batch, shaders->name##_cso, &constant_buffer,   \
+                               nr_blocks);
 
-        panfrost_update_shader_state(batch, PIPE_SHADER_COMPUTE);
+static void
+panfrost_afbc_size(struct panfrost_batch *batch, struct panfrost_resource *src,
+                   struct panfrost_bo *metadata, unsigned offset,
+                   unsigned level)
+{
+   struct pan_image_slice_layout *slice = &src->image.layout.slices[level];
+   struct panfrost_afbc_size_info consts = {
+      .src =
+         src->image.data.bo->ptr.gpu + src->image.data.offset + slice->offset,
+      .metadata = metadata->ptr.gpu + offset,
+   };
 
-#if PAN_ARCH <= 7
-        panfrost_pack_work_groups_compute(pan_section_ptr(t.cpu, COMPUTE_JOB, INVOCATION),
-                                          num_wg[0], num_wg[1], num_wg[2],
-                                          info->block[0], info->block[1],
-                                          info->block[2],
-                                          false, info->indirect != NULL);
-
-        pan_section_pack(t.cpu, COMPUTE_JOB, PARAMETERS, cfg) {
-                cfg.job_task_split =
-                        util_logbase2_ceil(info->block[0] + 1) +
-                        util_logbase2_ceil(info->block[1] + 1) +
-                        util_logbase2_ceil(info->block[2] + 1);
-        }
-
-        pan_section_pack(t.cpu, COMPUTE_JOB, DRAW, cfg) {
-                cfg.state = batch->rsd[PIPE_SHADER_COMPUTE];
-                cfg.attributes = panfrost_emit_image_attribs(batch, &cfg.attribute_buffers, PIPE_SHADER_COMPUTE);
-                cfg.thread_storage = panfrost_emit_shared_memory(batch, info);
-                cfg.uniform_buffers = batch->uniform_buffers[PIPE_SHADER_COMPUTE];
-                cfg.push_uniforms = batch->push_uniforms[PIPE_SHADER_COMPUTE];
-                cfg.textures = batch->textures[PIPE_SHADER_COMPUTE];
-                cfg.samplers = batch->samplers[PIPE_SHADER_COMPUTE];
-        }
-#else
-        struct panfrost_compiled_shader *cs = ctx->prog[PIPE_SHADER_COMPUTE];
-
-        pan_section_pack(t.cpu, COMPUTE_JOB, PAYLOAD, cfg) {
-                cfg.workgroup_size_x = info->block[0];
-                cfg.workgroup_size_y = info->block[1];
-                cfg.workgroup_size_z = info->block[2];
-
-                cfg.workgroup_count_x = num_wg[0];
-                cfg.workgroup_count_y = num_wg[1];
-                cfg.workgroup_count_z = num_wg[2];
-
-                panfrost_emit_shader(batch, &cfg.compute, PIPE_SHADER_COMPUTE,
-                                     batch->rsd[PIPE_SHADER_COMPUTE],
-                                     panfrost_emit_shared_memory(batch, info));
-
-                /* Workgroups may be merged if the shader does not use barriers
-                 * or shared memory. This condition is checked against the
-                 * static shared_size at compile-time. We need to check the
-                 * variable shared size at launch_grid time, because the
-                 * compiler doesn't know about that.
-                 */
-                cfg.allow_merging_workgroups =
-                        cs->info.cs.allow_merging_workgroups &&
-                        (info->variable_shared_mem == 0);
-
-                cfg.task_increment = 1;
-                cfg.task_axis = MALI_TASK_AXIS_Z;
-        }
-#endif
+   panfrost_batch_read_rsrc(batch, src, PIPE_SHADER_COMPUTE);
+   panfrost_batch_write_bo(batch, metadata, PIPE_SHADER_COMPUTE);
 
-        unsigned indirect_dep = 0;
-#if PAN_GPU_INDIRECTS
-        if (info->indirect) {
-                struct pan_indirect_dispatch_info indirect = {
-                        .job = t.gpu,
-                        .indirect_dim = pan_resource(info->indirect)->image.data.bo->ptr.gpu +
-                                        info->indirect_offset,
-                        .num_wg_sysval = {
-                                batch->num_wg_sysval[0],
-                                batch->num_wg_sysval[1],
-                                batch->num_wg_sysval[2],
-                        },
-                };
-
-                indirect_dep = GENX(pan_indirect_dispatch_emit)(&batch->pool.base,
-                                                                &batch->scoreboard,
-                                                                &indirect);
-        }
-#endif
+   LAUNCH_AFBC_SHADER(size, batch, src, consts, slice->afbc.nr_blocks);
+}
 
-        panfrost_add_job(&batch->pool.base, &batch->scoreboard,
-                         MALI_JOB_TYPE_COMPUTE, true, false,
-                         indirect_dep, 0, &t, false);
-        panfrost_flush_all_batches(ctx, "Launch grid post-barrier");
+static void
+panfrost_afbc_pack(struct panfrost_batch *batch, struct panfrost_resource *src,
+                   struct panfrost_bo *dst,
+                   struct pan_image_slice_layout *dst_slice,
+                   struct panfrost_bo *metadata, unsigned metadata_offset,
+                   unsigned level)
+{
+   struct pan_image_slice_layout *src_slice = &src->image.layout.slices[level];
+   struct panfrost_afbc_pack_info consts = {
+      .src = src->image.data.bo->ptr.gpu + src->image.data.offset +
+             src_slice->offset,
+      .dst = dst->ptr.gpu + dst_slice->offset,
+      .metadata = metadata->ptr.gpu + metadata_offset,
+      .header_size = dst_slice->afbc.header_size,
+      .src_stride = src_slice->afbc.stride,
+      .dst_stride = dst_slice->afbc.stride,
+   };
+
+   panfrost_batch_write_rsrc(batch, src, PIPE_SHADER_COMPUTE);
+   panfrost_batch_write_bo(batch, dst, PIPE_SHADER_COMPUTE);
+   panfrost_batch_add_bo(batch, metadata, PIPE_SHADER_COMPUTE);
+
+   LAUNCH_AFBC_SHADER(pack, batch, src, consts, dst_slice->afbc.nr_blocks);
 }
 
 static void *
-panfrost_create_rasterizer_state(
-        struct pipe_context *pctx,
-        const struct pipe_rasterizer_state *cso)
+panfrost_create_rasterizer_state(struct pipe_context *pctx,
+                                 const struct pipe_rasterizer_state *cso)
 {
-        struct panfrost_rasterizer *so = CALLOC_STRUCT(panfrost_rasterizer);
-
-        so->base = *cso;
+   struct panfrost_rasterizer *so = CALLOC_STRUCT(panfrost_rasterizer);
 
-        /* Gauranteed with the core GL call, so don't expose ARB_polygon_offset */
-        assert(cso->offset_clamp == 0.0);
+   so->base = *cso;
 
 #if PAN_ARCH <= 7
-        pan_pack(&so->multisample, MULTISAMPLE_MISC, cfg) {
-                cfg.multisample_enable = cso->multisample;
-                cfg.fixed_function_near_discard = cso->depth_clip_near;
-                cfg.fixed_function_far_discard = cso->depth_clip_far;
-                cfg.shader_depth_range_fixed = true;
-        }
-
-        pan_pack(&so->stencil_misc, STENCIL_MASK_MISC, cfg) {
-                cfg.front_facing_depth_bias = cso->offset_tri;
-                cfg.back_facing_depth_bias = cso->offset_tri;
-                cfg.single_sampled_lines = !cso->multisample;
-        }
+   pan_pack(&so->multisample, MULTISAMPLE_MISC, cfg) {
+      cfg.multisample_enable = cso->multisample;
+      cfg.fixed_function_near_discard = cso->depth_clip_near;
+      cfg.fixed_function_far_discard = cso->depth_clip_far;
+      cfg.shader_depth_range_fixed = true;
+   }
+
+   pan_pack(&so->stencil_misc, STENCIL_MASK_MISC, cfg) {
+      cfg.front_facing_depth_bias = cso->offset_tri;
+      cfg.back_facing_depth_bias = cso->offset_tri;
+      cfg.single_sampled_lines = !cso->multisample;
+   }
 #endif
 
-        return so;
+   return so;
 }
 
 #if PAN_ARCH >= 9
 /*
  * Given a pipe_vertex_element, pack the corresponding Valhall attribute
- * descriptor. This function is called at CSO create time. Since
- * pipe_vertex_element lacks a stride, the packed attribute descriptor will not
- * be uploaded until draw time.
+ * descriptor. This function is called at CSO create time.
  */
 static void
 panfrost_pack_attribute(struct panfrost_device *dev,
                         const struct pipe_vertex_element el,
                         struct mali_attribute_packed *out)
 {
-        pan_pack(out, ATTRIBUTE, cfg) {
-                cfg.table = PAN_TABLE_ATTRIBUTE_BUFFER;
-                cfg.frequency = (el.instance_divisor > 0) ?
-                        MALI_ATTRIBUTE_FREQUENCY_INSTANCE :
-                        MALI_ATTRIBUTE_FREQUENCY_VERTEX;
-                cfg.format = dev->formats[el.src_format].hw;
-                cfg.offset = el.src_offset;
-                cfg.buffer_index = el.vertex_buffer_index;
-
-                if (el.instance_divisor == 0) {
-                        /* Per-vertex */
-                        cfg.attribute_type = MALI_ATTRIBUTE_TYPE_1D;
-                        cfg.frequency = MALI_ATTRIBUTE_FREQUENCY_VERTEX;
-                        cfg.offset_enable = true;
-                } else if (util_is_power_of_two_or_zero(el.instance_divisor)) {
-                        /* Per-instance, POT divisor */
-                        cfg.attribute_type = MALI_ATTRIBUTE_TYPE_1D_POT_DIVISOR;
-                        cfg.frequency = MALI_ATTRIBUTE_FREQUENCY_INSTANCE;
-                        cfg.divisor_r = __builtin_ctz(el.instance_divisor);
-                } else {
-                        /* Per-instance, NPOT divisor */
-                        cfg.attribute_type = MALI_ATTRIBUTE_TYPE_1D_NPOT_DIVISOR;
-                        cfg.frequency = MALI_ATTRIBUTE_FREQUENCY_INSTANCE;
-
-                        cfg.divisor_d =
-                                panfrost_compute_magic_divisor(el.instance_divisor,
-                                                &cfg.divisor_r, &cfg.divisor_e);
-                }
-        }
+   pan_pack(out, ATTRIBUTE, cfg) {
+      cfg.table = PAN_TABLE_ATTRIBUTE_BUFFER;
+      cfg.frequency = (el.instance_divisor > 0)
+                         ? MALI_ATTRIBUTE_FREQUENCY_INSTANCE
+                         : MALI_ATTRIBUTE_FREQUENCY_VERTEX;
+      cfg.format = dev->formats[el.src_format].hw;
+      cfg.offset = el.src_offset;
+      cfg.buffer_index = el.vertex_buffer_index;
+      cfg.stride = el.src_stride;
+
+      if (el.instance_divisor == 0) {
+         /* Per-vertex */
+         cfg.attribute_type = MALI_ATTRIBUTE_TYPE_1D;
+         cfg.frequency = MALI_ATTRIBUTE_FREQUENCY_VERTEX;
+         cfg.offset_enable = true;
+      } else if (util_is_power_of_two_or_zero(el.instance_divisor)) {
+         /* Per-instance, POT divisor */
+         cfg.attribute_type = MALI_ATTRIBUTE_TYPE_1D_POT_DIVISOR;
+         cfg.frequency = MALI_ATTRIBUTE_FREQUENCY_INSTANCE;
+         cfg.divisor_r = __builtin_ctz(el.instance_divisor);
+      } else {
+         /* Per-instance, NPOT divisor */
+         cfg.attribute_type = MALI_ATTRIBUTE_TYPE_1D_NPOT_DIVISOR;
+         cfg.frequency = MALI_ATTRIBUTE_FREQUENCY_INSTANCE;
+
+         cfg.divisor_d = panfrost_compute_magic_divisor(
+            el.instance_divisor, &cfg.divisor_r, &cfg.divisor_e);
+      }
+   }
 }
 #endif
 
 static void *
-panfrost_create_vertex_elements_state(
-        struct pipe_context *pctx,
-        unsigned num_elements,
-        const struct pipe_vertex_element *elements)
+panfrost_create_vertex_elements_state(struct pipe_context *pctx,
+                                      unsigned num_elements,
+                                      const struct pipe_vertex_element *elements)
 {
-        struct panfrost_vertex_state *so = CALLOC_STRUCT(panfrost_vertex_state);
-        struct panfrost_device *dev = pan_device(pctx->screen);
+   struct panfrost_vertex_state *so = CALLOC_STRUCT(panfrost_vertex_state);
+   struct panfrost_device *dev = pan_device(pctx->screen);
 
-        so->num_elements = num_elements;
-        memcpy(so->pipe, elements, sizeof(*elements) * num_elements);
+   so->num_elements = num_elements;
+   memcpy(so->pipe, elements, sizeof(*elements) * num_elements);
 
+   for (unsigned i = 0; i < num_elements; ++i)
+      so->strides[elements[i].vertex_buffer_index] = elements[i].src_stride;
 #if PAN_ARCH >= 9
-        for (unsigned i = 0; i < num_elements; ++i)
-                panfrost_pack_attribute(dev, elements[i], &so->attributes[i]);
+   for (unsigned i = 0; i < num_elements; ++i)
+      panfrost_pack_attribute(dev, elements[i], &so->attributes[i]);
 #else
-        /* Assign attribute buffers corresponding to the vertex buffers, keyed
-         * for a particular divisor since that's how instancing works on Mali */
-        for (unsigned i = 0; i < num_elements; ++i) {
-                so->element_buffer[i] = pan_assign_vertex_buffer(
-                                so->buffers, &so->nr_bufs,
-                                elements[i].vertex_buffer_index,
-                                elements[i].instance_divisor);
-        }
-
-        for (int i = 0; i < num_elements; ++i) {
-                enum pipe_format fmt = elements[i].src_format;
-                so->formats[i] = dev->formats[fmt].hw;
-        }
-
-        /* Let's also prepare vertex builtins */
-        so->formats[PAN_VERTEX_ID] = dev->formats[PIPE_FORMAT_R32_UINT].hw;
-        so->formats[PAN_INSTANCE_ID] = dev->formats[PIPE_FORMAT_R32_UINT].hw;
+   /* Assign attribute buffers corresponding to the vertex buffers, keyed
+    * for a particular divisor since that's how instancing works on Mali */
+   for (unsigned i = 0; i < num_elements; ++i) {
+      so->element_buffer[i] = pan_assign_vertex_buffer(
+         so->buffers, &so->nr_bufs, elements[i].vertex_buffer_index,
+         elements[i].instance_divisor);
+   }
+
+   for (int i = 0; i < num_elements; ++i) {
+      enum pipe_format fmt = elements[i].src_format;
+      so->formats[i] = dev->formats[fmt].hw;
+
+      assert(MALI_EXTRACT_INDEX(so->formats[i]) && "format must be supported");
+   }
+
+   /* Let's also prepare vertex builtins */
+   so->formats[PAN_VERTEX_ID] = dev->formats[PIPE_FORMAT_R32_UINT].hw;
+   so->formats[PAN_INSTANCE_ID] = dev->formats[PIPE_FORMAT_R32_UINT].hw;
 #endif
 
-        return so;
+   return so;
 }
 
 static inline unsigned
 pan_pipe_to_stencil_op(enum pipe_stencil_op in)
 {
-        switch (in) {
-        case PIPE_STENCIL_OP_KEEP: return MALI_STENCIL_OP_KEEP;
-        case PIPE_STENCIL_OP_ZERO: return MALI_STENCIL_OP_ZERO;
-        case PIPE_STENCIL_OP_REPLACE: return MALI_STENCIL_OP_REPLACE;
-        case PIPE_STENCIL_OP_INCR: return MALI_STENCIL_OP_INCR_SAT;
-        case PIPE_STENCIL_OP_DECR: return MALI_STENCIL_OP_DECR_SAT;
-        case PIPE_STENCIL_OP_INCR_WRAP: return MALI_STENCIL_OP_INCR_WRAP;
-        case PIPE_STENCIL_OP_DECR_WRAP: return MALI_STENCIL_OP_DECR_WRAP;
-        case PIPE_STENCIL_OP_INVERT: return MALI_STENCIL_OP_INVERT;
-        default: unreachable("Invalid stencil op");
-        }
+   switch (in) {
+   case PIPE_STENCIL_OP_KEEP:
+      return MALI_STENCIL_OP_KEEP;
+   case PIPE_STENCIL_OP_ZERO:
+      return MALI_STENCIL_OP_ZERO;
+   case PIPE_STENCIL_OP_REPLACE:
+      return MALI_STENCIL_OP_REPLACE;
+   case PIPE_STENCIL_OP_INCR:
+      return MALI_STENCIL_OP_INCR_SAT;
+   case PIPE_STENCIL_OP_DECR:
+      return MALI_STENCIL_OP_DECR_SAT;
+   case PIPE_STENCIL_OP_INCR_WRAP:
+      return MALI_STENCIL_OP_INCR_WRAP;
+   case PIPE_STENCIL_OP_DECR_WRAP:
+      return MALI_STENCIL_OP_DECR_WRAP;
+   case PIPE_STENCIL_OP_INVERT:
+      return MALI_STENCIL_OP_INVERT;
+   default:
+      unreachable("Invalid stencil op");
+   }
 }
 
 #if PAN_ARCH <= 7
@@ -4330,127 +3264,127 @@ static inline void
 pan_pipe_to_stencil(const struct pipe_stencil_state *in,
                     struct mali_stencil_packed *out)
 {
-        pan_pack(out, STENCIL, s) {
-                s.mask = in->valuemask;
-                s.compare_function = (enum mali_func) in->func;
-                s.stencil_fail = pan_pipe_to_stencil_op(in->fail_op);
-                s.depth_fail = pan_pipe_to_stencil_op(in->zfail_op);
-                s.depth_pass = pan_pipe_to_stencil_op(in->zpass_op);
-        }
+   pan_pack(out, STENCIL, s) {
+      s.mask = in->valuemask;
+      s.compare_function = (enum mali_func)in->func;
+      s.stencil_fail = pan_pipe_to_stencil_op(in->fail_op);
+      s.depth_fail = pan_pipe_to_stencil_op(in->zfail_op);
+      s.depth_pass = pan_pipe_to_stencil_op(in->zpass_op);
+   }
 }
 #endif
 
 static bool
 pipe_zs_always_passes(const struct pipe_depth_stencil_alpha_state *zsa)
 {
-        if (zsa->depth_enabled && zsa->depth_func != PIPE_FUNC_ALWAYS)
-                return false;
+   if (zsa->depth_enabled && zsa->depth_func != PIPE_FUNC_ALWAYS)
+      return false;
 
-        if (zsa->stencil[0].enabled && zsa->stencil[0].func != PIPE_FUNC_ALWAYS)
-                return false;
+   if (zsa->stencil[0].enabled && zsa->stencil[0].func != PIPE_FUNC_ALWAYS)
+      return false;
 
-        if (zsa->stencil[1].enabled && zsa->stencil[1].func != PIPE_FUNC_ALWAYS)
-                return false;
+   if (zsa->stencil[1].enabled && zsa->stencil[1].func != PIPE_FUNC_ALWAYS)
+      return false;
 
-        return true;
+   return true;
 }
 
 static void *
-panfrost_create_depth_stencil_state(struct pipe_context *pipe,
-                                    const struct pipe_depth_stencil_alpha_state *zsa)
+panfrost_create_depth_stencil_state(
+   struct pipe_context *pipe, const struct pipe_depth_stencil_alpha_state *zsa)
 {
-        struct panfrost_zsa_state *so = CALLOC_STRUCT(panfrost_zsa_state);
-        so->base = *zsa;
+   struct panfrost_zsa_state *so = CALLOC_STRUCT(panfrost_zsa_state);
+   so->base = *zsa;
 
-        const struct pipe_stencil_state front = zsa->stencil[0];
-        const struct pipe_stencil_state back =
-                zsa->stencil[1].enabled ? zsa->stencil[1] : front;
+   const struct pipe_stencil_state front = zsa->stencil[0];
+   const struct pipe_stencil_state back =
+      zsa->stencil[1].enabled ? zsa->stencil[1] : front;
 
-        enum mali_func depth_func = zsa->depth_enabled ?
-                (enum mali_func) zsa->depth_func : MALI_FUNC_ALWAYS;
+   enum mali_func depth_func =
+      zsa->depth_enabled ? (enum mali_func)zsa->depth_func : MALI_FUNC_ALWAYS;
 
-        /* Normalize (there's no separate enable) */
-        if (PAN_ARCH <= 5 && !zsa->alpha_enabled)
-                so->base.alpha_func = MALI_FUNC_ALWAYS;
+   /* Normalize (there's no separate enable) */
+   if (PAN_ARCH <= 5 && !zsa->alpha_enabled)
+      so->base.alpha_func = MALI_FUNC_ALWAYS;
 
 #if PAN_ARCH <= 7
-        /* Prepack relevant parts of the Renderer State Descriptor. They will
-         * be ORed in at draw-time */
-        pan_pack(&so->rsd_depth, MULTISAMPLE_MISC, cfg) {
-                cfg.depth_function = depth_func;
-                cfg.depth_write_mask = zsa->depth_writemask;
-        }
-
-        pan_pack(&so->rsd_stencil, STENCIL_MASK_MISC, cfg) {
-                cfg.stencil_enable = front.enabled;
-                cfg.stencil_mask_front = front.writemask;
-                cfg.stencil_mask_back = back.writemask;
+   /* Prepack relevant parts of the Renderer State Descriptor. They will
+    * be ORed in at draw-time */
+   pan_pack(&so->rsd_depth, MULTISAMPLE_MISC, cfg) {
+      cfg.depth_function = depth_func;
+      cfg.depth_write_mask = zsa->depth_writemask;
+   }
+
+   pan_pack(&so->rsd_stencil, STENCIL_MASK_MISC, cfg) {
+      cfg.stencil_enable = front.enabled;
+      cfg.stencil_mask_front = front.writemask;
+      cfg.stencil_mask_back = back.writemask;
 
 #if PAN_ARCH <= 5
-                cfg.alpha_test_compare_function =
-                        (enum mali_func) so->base.alpha_func;
+      cfg.alpha_test_compare_function = (enum mali_func)so->base.alpha_func;
 #endif
-        }
+   }
 
-        /* Stencil tests have their own words in the RSD */
-        pan_pipe_to_stencil(&front, &so->stencil_front);
-        pan_pipe_to_stencil(&back, &so->stencil_back);
+   /* Stencil tests have their own words in the RSD */
+   pan_pipe_to_stencil(&front, &so->stencil_front);
+   pan_pipe_to_stencil(&back, &so->stencil_back);
 #else
-        pan_pack(&so->desc, DEPTH_STENCIL, cfg) {
-                cfg.front_compare_function = (enum mali_func) front.func;
-                cfg.front_stencil_fail = pan_pipe_to_stencil_op(front.fail_op);
-                cfg.front_depth_fail = pan_pipe_to_stencil_op(front.zfail_op);
-                cfg.front_depth_pass = pan_pipe_to_stencil_op(front.zpass_op);
-
-                cfg.back_compare_function = (enum mali_func) back.func;
-                cfg.back_stencil_fail = pan_pipe_to_stencil_op(back.fail_op);
-                cfg.back_depth_fail = pan_pipe_to_stencil_op(back.zfail_op);
-                cfg.back_depth_pass = pan_pipe_to_stencil_op(back.zpass_op);
-
-                cfg.stencil_test_enable = front.enabled;
-                cfg.front_write_mask = front.writemask;
-                cfg.back_write_mask = back.writemask;
-                cfg.front_value_mask = front.valuemask;
-                cfg.back_value_mask = back.valuemask;
-
-                cfg.depth_write_enable = zsa->depth_writemask;
-                cfg.depth_function = depth_func;
-        }
+   pan_pack(&so->desc, DEPTH_STENCIL, cfg) {
+      cfg.front_compare_function = (enum mali_func)front.func;
+      cfg.front_stencil_fail = pan_pipe_to_stencil_op(front.fail_op);
+      cfg.front_depth_fail = pan_pipe_to_stencil_op(front.zfail_op);
+      cfg.front_depth_pass = pan_pipe_to_stencil_op(front.zpass_op);
+
+      cfg.back_compare_function = (enum mali_func)back.func;
+      cfg.back_stencil_fail = pan_pipe_to_stencil_op(back.fail_op);
+      cfg.back_depth_fail = pan_pipe_to_stencil_op(back.zfail_op);
+      cfg.back_depth_pass = pan_pipe_to_stencil_op(back.zpass_op);
+
+      cfg.stencil_test_enable = front.enabled;
+      cfg.front_write_mask = front.writemask;
+      cfg.back_write_mask = back.writemask;
+      cfg.front_value_mask = front.valuemask;
+      cfg.back_value_mask = back.valuemask;
+
+      cfg.depth_write_enable = zsa->depth_writemask;
+      cfg.depth_function = depth_func;
+   }
 #endif
 
-        so->enabled = zsa->stencil[0].enabled ||
-                (zsa->depth_enabled && zsa->depth_func != PIPE_FUNC_ALWAYS);
+   so->enabled = zsa->stencil[0].enabled ||
+                 (zsa->depth_enabled && zsa->depth_func != PIPE_FUNC_ALWAYS);
 
-        so->zs_always_passes = pipe_zs_always_passes(zsa);
-        so->writes_zs = util_writes_depth_stencil(zsa);
+   so->zs_always_passes = pipe_zs_always_passes(zsa);
+   so->writes_zs = util_writes_depth_stencil(zsa);
 
-        /* TODO: Bounds test should be easy */
-        assert(!zsa->depth_bounds_test);
+   /* TODO: Bounds test should be easy */
+   assert(!zsa->depth_bounds_test);
 
-        return so;
+   return so;
 }
 
 static struct pipe_sampler_view *
-panfrost_create_sampler_view(
-        struct pipe_context *pctx,
-        struct pipe_resource *texture,
-        const struct pipe_sampler_view *template)
+panfrost_create_sampler_view(struct pipe_context *pctx,
+                             struct pipe_resource *texture,
+                             const struct pipe_sampler_view *template)
 {
-        struct panfrost_context *ctx = pan_context(pctx);
-        struct panfrost_sampler_view *so = rzalloc(pctx, struct panfrost_sampler_view);
+   struct panfrost_context *ctx = pan_context(pctx);
+   struct panfrost_sampler_view *so =
+      rzalloc(pctx, struct panfrost_sampler_view);
 
-        pan_legalize_afbc_format(ctx, pan_resource(texture), template->format);
+   pan_legalize_afbc_format(ctx, pan_resource(texture), template->format,
+                            false);
 
-        pipe_reference(NULL, &texture->reference);
+   pipe_reference(NULL, &texture->reference);
 
-        so->base = *template;
-        so->base.texture = texture;
-        so->base.reference.count = 1;
-        so->base.context = pctx;
+   so->base = *template;
+   so->base.texture = texture;
+   so->base.reference.count = 1;
+   so->base.context = pctx;
 
-        panfrost_create_sampler_view_bo(so, pctx, texture);
+   panfrost_create_sampler_view_bo(so, pctx, texture);
 
-        return (struct pipe_sampler_view *) so;
+   return (struct pipe_sampler_view *)so;
 }
 
 /* A given Gallium blend state can be encoded to the hardware in numerous,
@@ -4485,257 +3419,217 @@ static void *
 panfrost_create_blend_state(struct pipe_context *pipe,
                             const struct pipe_blend_state *blend)
 {
-        struct panfrost_blend_state *so = CALLOC_STRUCT(panfrost_blend_state);
-        so->base = *blend;
-
-        so->pan.logicop_enable = blend->logicop_enable;
-        so->pan.logicop_func = blend->logicop_func;
-        so->pan.rt_count = blend->max_rt + 1;
-
-        for (unsigned c = 0; c < so->pan.rt_count; ++c) {
-                unsigned g = blend->independent_blend_enable ? c : 0;
-                const struct pipe_rt_blend_state pipe = blend->rt[g];
-                struct pan_blend_equation equation = {0};
-
-                equation.color_mask = pipe.colormask;
-                equation.blend_enable = pipe.blend_enable;
-
-                if (pipe.blend_enable) {
-                        equation.rgb_func = util_blend_func_to_shader(pipe.rgb_func);
-                        equation.rgb_src_factor = util_blend_factor_to_shader(pipe.rgb_src_factor);
-                        equation.rgb_invert_src_factor = util_blend_factor_is_inverted(pipe.rgb_src_factor);
-                        equation.rgb_dst_factor = util_blend_factor_to_shader(pipe.rgb_dst_factor);
-                        equation.rgb_invert_dst_factor = util_blend_factor_is_inverted(pipe.rgb_dst_factor);
-                        equation.alpha_func = util_blend_func_to_shader(pipe.alpha_func);
-                        equation.alpha_src_factor = util_blend_factor_to_shader(pipe.alpha_src_factor);
-                        equation.alpha_invert_src_factor = util_blend_factor_is_inverted(pipe.alpha_src_factor);
-                        equation.alpha_dst_factor = util_blend_factor_to_shader(pipe.alpha_dst_factor);
-                        equation.alpha_invert_dst_factor = util_blend_factor_is_inverted(pipe.alpha_dst_factor);
-                }
-
-                /* Determine some common properties */
-                unsigned constant_mask = pan_blend_constant_mask(equation);
-                const bool supports_2src = pan_blend_supports_2src(PAN_ARCH);
-                so->info[c] = (struct pan_blend_info) {
-                        .enabled = (equation.color_mask != 0),
-                        .opaque = pan_blend_is_opaque(equation),
-                        .constant_mask = constant_mask,
-
-                        /* TODO: check the dest for the logicop */
-                        .load_dest = blend->logicop_enable ||
-                                pan_blend_reads_dest(equation),
-
-                        /* Could this possibly be fixed-function? */
-                        .fixed_function = !blend->logicop_enable &&
-                                pan_blend_can_fixed_function(equation,
-                                                             supports_2src) &&
-                                (!constant_mask ||
-                                 pan_blend_supports_constant(PAN_ARCH, c)),
-
-                        .alpha_zero_nop = pan_blend_alpha_zero_nop(equation),
-                        .alpha_one_store = pan_blend_alpha_one_store(equation),
-                };
-
-                so->pan.rts[c].equation = equation;
-
-                /* Bifrost needs to know if any render target loads its
-                 * destination in the hot draw path, so precompute this */
-                if (so->info[c].load_dest)
-                        so->load_dest_mask |= BITFIELD_BIT(c);
-
-                /* Converting equations to Mali style is expensive, do it at
-                 * CSO create time instead of draw-time */
-                if (so->info[c].fixed_function) {
-                        so->equation[c] = pan_pack_blend(equation);
-                }
-        }
-
-        return so;
+   struct panfrost_blend_state *so = CALLOC_STRUCT(panfrost_blend_state);
+   so->base = *blend;
+
+   so->pan.logicop_enable = blend->logicop_enable;
+   so->pan.logicop_func = blend->logicop_func;
+   so->pan.rt_count = blend->max_rt + 1;
+
+   for (unsigned c = 0; c < so->pan.rt_count; ++c) {
+      unsigned g = blend->independent_blend_enable ? c : 0;
+      const struct pipe_rt_blend_state pipe = blend->rt[g];
+      struct pan_blend_equation equation = {0};
+
+      equation.color_mask = pipe.colormask;
+      equation.blend_enable = pipe.blend_enable;
+
+      if (pipe.blend_enable) {
+         equation.rgb_func = pipe.rgb_func;
+         equation.rgb_src_factor = pipe.rgb_src_factor;
+         equation.rgb_dst_factor = pipe.rgb_dst_factor;
+         equation.alpha_func = pipe.alpha_func;
+         equation.alpha_src_factor = pipe.alpha_src_factor;
+         equation.alpha_dst_factor = pipe.alpha_dst_factor;
+      }
+
+      /* Determine some common properties */
+      unsigned constant_mask = pan_blend_constant_mask(equation);
+      const bool supports_2src = pan_blend_supports_2src(PAN_ARCH);
+      so->info[c] = (struct pan_blend_info){
+         .enabled = (equation.color_mask != 0) &&
+                    !(blend->logicop_enable &&
+                      blend->logicop_func == PIPE_LOGICOP_NOOP),
+         .opaque = !blend->logicop_enable && pan_blend_is_opaque(equation),
+         .constant_mask = constant_mask,
+
+         /* TODO: check the dest for the logicop */
+         .load_dest = blend->logicop_enable || pan_blend_reads_dest(equation),
+
+         /* Could this possibly be fixed-function? */
+         .fixed_function =
+            !blend->logicop_enable &&
+            pan_blend_can_fixed_function(equation, supports_2src) &&
+            (!constant_mask || pan_blend_supports_constant(PAN_ARCH, c)),
+
+         .alpha_zero_nop = pan_blend_alpha_zero_nop(equation),
+         .alpha_one_store = pan_blend_alpha_one_store(equation),
+      };
+
+      so->pan.rts[c].equation = equation;
+
+      /* Bifrost needs to know if any render target loads its
+       * destination in the hot draw path, so precompute this */
+      if (so->info[c].load_dest)
+         so->load_dest_mask |= BITFIELD_BIT(c);
+
+      /* Bifrost needs to know if any render target loads its
+       * destination in the hot draw path, so precompute this */
+      if (so->info[c].enabled)
+         so->enabled_mask |= BITFIELD_BIT(c);
+
+      /* Converting equations to Mali style is expensive, do it at
+       * CSO create time instead of draw-time */
+      if (so->info[c].fixed_function) {
+         so->equation[c] = pan_pack_blend(equation);
+      }
+   }
+
+   return so;
 }
 
 #if PAN_ARCH >= 9
 static enum mali_flush_to_zero_mode
 panfrost_ftz_mode(struct pan_shader_info *info)
 {
-        if (info->ftz_fp32) {
-                if (info->ftz_fp16)
-                        return MALI_FLUSH_TO_ZERO_MODE_ALWAYS;
-                else
-                        return MALI_FLUSH_TO_ZERO_MODE_DX11;
-        } else {
-                /* We don't have a "flush FP16, preserve FP32" mode, but APIs
-                 * should not be able to generate that.
-                 */
-                assert(!info->ftz_fp16 && !info->ftz_fp32);
-                return MALI_FLUSH_TO_ZERO_MODE_PRESERVE_SUBNORMALS;
-        }
+   if (info->ftz_fp32) {
+      if (info->ftz_fp16)
+         return MALI_FLUSH_TO_ZERO_MODE_ALWAYS;
+      else
+         return MALI_FLUSH_TO_ZERO_MODE_DX11;
+   } else {
+      /* We don't have a "flush FP16, preserve FP32" mode, but APIs
+       * should not be able to generate that.
+       */
+      assert(!info->ftz_fp16 && !info->ftz_fp32);
+      return MALI_FLUSH_TO_ZERO_MODE_PRESERVE_SUBNORMALS;
+   }
 }
 #endif
 
 static void
 prepare_shader(struct panfrost_compiled_shader *state,
-            struct panfrost_pool *pool, bool upload)
+               struct panfrost_pool *pool, bool upload)
 {
 #if PAN_ARCH <= 7
-        void *out = &state->partial_rsd;
+   void *out = &state->partial_rsd;
 
-        if (upload) {
-                struct panfrost_ptr ptr =
-                        pan_pool_alloc_desc(&pool->base, RENDERER_STATE);
+   if (upload) {
+      struct panfrost_ptr ptr =
+         pan_pool_alloc_desc(&pool->base, RENDERER_STATE);
 
-                state->state = panfrost_pool_take_ref(pool, ptr.gpu);
-                out = ptr.cpu;
-        }
+      state->state = panfrost_pool_take_ref(pool, ptr.gpu);
+      out = ptr.cpu;
+   }
 
-        pan_pack(out, RENDERER_STATE, cfg) {
-                pan_shader_prepare_rsd(&state->info, state->bin.gpu, &cfg);
-
-       }
+   pan_pack(out, RENDERER_STATE, cfg) {
+      pan_shader_prepare_rsd(&state->info, state->bin.gpu, &cfg);
+   }
 #else
-        assert(upload);
-
-        /* The address in the shader program descriptor must be non-null, but
-         * the entire shader program descriptor may be omitted.
-         *
-         * See dEQP-GLES31.functional.compute.basic.empty
-         */
-        if (!state->bin.gpu)
-                return;
-
-        bool vs = (state->info.stage == MESA_SHADER_VERTEX);
-        bool secondary_enable = (vs && state->info.vs.secondary_enable);
-
-        unsigned nr_variants = secondary_enable ? 3 : vs ? 2 : 1;
-        struct panfrost_ptr ptr = pan_pool_alloc_desc_array(&pool->base,
-                                                            nr_variants,
-                                                            SHADER_PROGRAM);
-
-        state->state = panfrost_pool_take_ref(pool, ptr.gpu);
-
-        /* Generic, or IDVS/points */
-        pan_pack(ptr.cpu, SHADER_PROGRAM, cfg) {
-                cfg.stage = pan_shader_stage(&state->info);
-                cfg.primary_shader = true;
-                cfg.register_allocation = pan_register_allocation(state->info.work_reg_count);
-                cfg.binary = state->bin.gpu;
-                cfg.preload.r48_r63 = (state->info.preload >> 48);
-                cfg.flush_to_zero_mode = panfrost_ftz_mode(&state->info);
-
-                if (cfg.stage == MALI_SHADER_STAGE_FRAGMENT)
-                        cfg.requires_helper_threads = state->info.contains_barrier;
-        }
-
-        if (!vs)
-                return;
-
-        /* IDVS/triangles */
-        pan_pack(ptr.cpu + pan_size(SHADER_PROGRAM), SHADER_PROGRAM, cfg) {
-                cfg.stage = pan_shader_stage(&state->info);
-                cfg.primary_shader = true;
-                cfg.register_allocation = pan_register_allocation(state->info.work_reg_count);
-                cfg.binary = state->bin.gpu + state->info.vs.no_psiz_offset;
-                cfg.preload.r48_r63 = (state->info.preload >> 48);
-                cfg.flush_to_zero_mode = panfrost_ftz_mode(&state->info);
-        }
-
-        if (!secondary_enable)
-                return;
-
-        pan_pack(ptr.cpu + (pan_size(SHADER_PROGRAM) * 2), SHADER_PROGRAM, cfg) {
-                unsigned work_count = state->info.vs.secondary_work_reg_count;
-
-                cfg.stage = pan_shader_stage(&state->info);
-                cfg.primary_shader = false;
-                cfg.register_allocation = pan_register_allocation(work_count);
-                cfg.binary = state->bin.gpu + state->info.vs.secondary_offset;
-                cfg.preload.r48_r63 = (state->info.vs.secondary_preload >> 48);
-                cfg.flush_to_zero_mode = panfrost_ftz_mode(&state->info);
-        }
+   assert(upload);
+
+   /* The address in the shader program descriptor must be non-null, but
+    * the entire shader program descriptor may be omitted.
+    *
+    * See dEQP-GLES31.functional.compute.basic.empty
+    */
+   if (!state->bin.gpu)
+      return;
+
+   bool vs = (state->info.stage == MESA_SHADER_VERTEX);
+   bool secondary_enable = (vs && state->info.vs.secondary_enable);
+
+   unsigned nr_variants = secondary_enable ? 3 : vs ? 2 : 1;
+   struct panfrost_ptr ptr =
+      pan_pool_alloc_desc_array(&pool->base, nr_variants, SHADER_PROGRAM);
+
+   state->state = panfrost_pool_take_ref(pool, ptr.gpu);
+
+   /* Generic, or IDVS/points */
+   pan_pack(ptr.cpu, SHADER_PROGRAM, cfg) {
+      cfg.stage = pan_shader_stage(&state->info);
+
+      if (cfg.stage == MALI_SHADER_STAGE_FRAGMENT)
+         cfg.fragment_coverage_bitmask_type = MALI_COVERAGE_BITMASK_TYPE_GL;
+      else if (vs)
+         cfg.vertex_warp_limit = MALI_WARP_LIMIT_HALF;
+
+      cfg.register_allocation =
+         pan_register_allocation(state->info.work_reg_count);
+      cfg.binary = state->bin.gpu;
+      cfg.preload.r48_r63 = (state->info.preload >> 48);
+      cfg.flush_to_zero_mode = panfrost_ftz_mode(&state->info);
+
+      if (cfg.stage == MALI_SHADER_STAGE_FRAGMENT)
+         cfg.requires_helper_threads = state->info.contains_barrier;
+   }
+
+   if (!vs)
+      return;
+
+   /* IDVS/triangles */
+   pan_pack(ptr.cpu + pan_size(SHADER_PROGRAM), SHADER_PROGRAM, cfg) {
+      cfg.stage = pan_shader_stage(&state->info);
+      cfg.vertex_warp_limit = MALI_WARP_LIMIT_HALF;
+      cfg.register_allocation =
+         pan_register_allocation(state->info.work_reg_count);
+      cfg.binary = state->bin.gpu + state->info.vs.no_psiz_offset;
+      cfg.preload.r48_r63 = (state->info.preload >> 48);
+      cfg.flush_to_zero_mode = panfrost_ftz_mode(&state->info);
+   }
+
+   if (!secondary_enable)
+      return;
+
+   pan_pack(ptr.cpu + (pan_size(SHADER_PROGRAM) * 2), SHADER_PROGRAM, cfg) {
+      unsigned work_count = state->info.vs.secondary_work_reg_count;
+
+      cfg.stage = pan_shader_stage(&state->info);
+      cfg.vertex_warp_limit = MALI_WARP_LIMIT_FULL;
+      cfg.register_allocation = pan_register_allocation(work_count);
+      cfg.binary = state->bin.gpu + state->info.vs.secondary_offset;
+      cfg.preload.r48_r63 = (state->info.vs.secondary_preload >> 48);
+      cfg.flush_to_zero_mode = panfrost_ftz_mode(&state->info);
+   }
 #endif
 }
 
-static void
-panfrost_get_sample_position(struct pipe_context *context,
-                             unsigned sample_count,
-                             unsigned sample_index,
-                             float *out_value)
-{
-        panfrost_query_sample_position(
-                        panfrost_sample_pattern(sample_count),
-                        sample_index,
-                        out_value);
-}
-
 static void
 screen_destroy(struct pipe_screen *pscreen)
 {
-        struct panfrost_device *dev = pan_device(pscreen);
-        GENX(pan_blitter_cleanup)(dev);
-
+   struct panfrost_device *dev = pan_device(pscreen);
+   GENX(pan_blitter_cleanup)(dev);
 #if PAN_GPU_INDIRECTS
-        GENX(panfrost_cleanup_indirect_draw_shaders)(dev);
-        GENX(pan_indirect_dispatch_cleanup)(dev);
+   GENX(pan_indirect_dispatch_cleanup)(dev);
 #endif
 }
 
 static void
-preload(struct panfrost_batch *batch, struct pan_fb_info *fb)
-{
-        GENX(pan_preload_fb)(&batch->pool.base, &batch->scoreboard, fb, batch->tls.gpu,
-                             PAN_ARCH >= 6 ? batch->tiler_ctx.bifrost : 0, NULL);
-}
-
-static void
-init_batch(struct panfrost_batch *batch)
+panfrost_sampler_view_destroy(struct pipe_context *pctx,
+                              struct pipe_sampler_view *pview)
 {
-        /* Reserve the framebuffer and local storage descriptors */
-        batch->framebuffer =
-#if PAN_ARCH == 4
-                pan_pool_alloc_desc(&batch->pool.base, FRAMEBUFFER);
-#else
-                pan_pool_alloc_desc_aggregate(&batch->pool.base,
-                                              PAN_DESC(FRAMEBUFFER),
-                                              PAN_DESC(ZS_CRC_EXTENSION),
-                                              PAN_DESC_ARRAY(MAX2(batch->key.nr_cbufs, 1), RENDER_TARGET));
-
-                batch->framebuffer.gpu |= MALI_FBD_TAG_IS_MFBD;
-#endif
+   struct panfrost_sampler_view *view = (struct panfrost_sampler_view *)pview;
 
-#if PAN_ARCH >= 6
-        batch->tls = pan_pool_alloc_desc(&batch->pool.base, LOCAL_STORAGE);
-#else
-        /* On Midgard, the TLS is embedded in the FB descriptor */
-        batch->tls = batch->framebuffer;
-#endif
+   pipe_resource_reference(&pview->texture, NULL);
+   panfrost_bo_unreference(view->state.bo);
+   ralloc_free(view);
 }
 
 static void
-panfrost_sampler_view_destroy(
-        struct pipe_context *pctx,
-        struct pipe_sampler_view *pview)
+context_populate_vtbl(struct pipe_context *pipe)
 {
-        struct panfrost_sampler_view *view = (struct panfrost_sampler_view *) pview;
+   pipe->draw_vbo = panfrost_draw_vbo;
+   pipe->launch_grid = panfrost_launch_grid;
 
-        pipe_resource_reference(&pview->texture, NULL);
-        panfrost_bo_unreference(view->state.bo);
-        ralloc_free(view);
-}
+   pipe->create_vertex_elements_state = panfrost_create_vertex_elements_state;
+   pipe->create_rasterizer_state = panfrost_create_rasterizer_state;
+   pipe->create_depth_stencil_alpha_state = panfrost_create_depth_stencil_state;
+   pipe->create_sampler_view = panfrost_create_sampler_view;
+   pipe->sampler_view_destroy = panfrost_sampler_view_destroy;
+   pipe->create_sampler_state = panfrost_create_sampler_state;
+   pipe->create_blend_state = panfrost_create_blend_state;
 
-static void
-context_init(struct pipe_context *pipe)
-{
-        pipe->draw_vbo           = panfrost_draw_vbo;
-        pipe->launch_grid        = panfrost_launch_grid;
-
-        pipe->create_vertex_elements_state = panfrost_create_vertex_elements_state;
-        pipe->create_rasterizer_state = panfrost_create_rasterizer_state;
-        pipe->create_depth_stencil_alpha_state = panfrost_create_depth_stencil_state;
-        pipe->create_sampler_view = panfrost_create_sampler_view;
-        pipe->sampler_view_destroy = panfrost_sampler_view_destroy;
-        pipe->create_sampler_state = panfrost_create_sampler_state;
-        pipe->create_blend_state = panfrost_create_blend_state;
-
-        pipe->get_sample_position = panfrost_get_sample_position;
+   pipe->get_sample_position = u_default_get_sample_position;
 }
 
 #if PAN_ARCH <= 5
@@ -4797,34 +3685,47 @@ static void
 init_polygon_list(struct panfrost_batch *batch)
 {
 #if PAN_ARCH <= 5
-        mali_ptr polygon_list = batch_get_polygon_list(batch);
-        panfrost_scoreboard_initialize_tiler(&batch->pool.base,
-                                             &batch->scoreboard,
-                                             polygon_list);
+   mali_ptr polygon_list = batch_get_polygon_list(batch);
+   pan_jc_initialize_tiler(&batch->pool.base, &batch->jm.jobs.vtc_jc,
+                           polygon_list);
 #endif
 }
 
+static int
+submit_batch(struct panfrost_batch *batch, struct pan_fb_info *fb)
+{
+   JOBX(preload_fb)(batch, fb);
+   init_polygon_list(batch);
+
+   /* Now that all draws are in, we can finally prepare the
+    * FBD for the batch (if there is one). */
+
+   emit_tls(batch);
+
+   if (panfrost_has_fragment_job(batch)) {
+      emit_fbd(batch, fb);
+      emit_fragment_job(batch, fb);
+   }
+
+   return JOBX(submit_batch)(batch);
+}
+
 void
 GENX(panfrost_cmdstream_screen_init)(struct panfrost_screen *screen)
 {
-        struct panfrost_device *dev = &screen->dev;
-
-        screen->vtbl.prepare_shader = prepare_shader;
-        screen->vtbl.emit_tls    = emit_tls;
-        screen->vtbl.emit_fbd    = emit_fbd;
-        screen->vtbl.emit_fragment_job = emit_fragment_job;
-        screen->vtbl.screen_destroy = screen_destroy;
-        screen->vtbl.preload     = preload;
-        screen->vtbl.context_init = context_init;
-        screen->vtbl.init_batch = init_batch;
-        screen->vtbl.get_blend_shader = GENX(pan_blend_get_shader_locked);
-        screen->vtbl.init_polygon_list = init_polygon_list;
-        screen->vtbl.get_compiler_options = GENX(pan_shader_get_compiler_options);
-        screen->vtbl.compile_shader = GENX(pan_shader_compile);
-
-        GENX(pan_blitter_init)(dev, &screen->blitter.bin_pool.base,
-                               &screen->blitter.desc_pool.base);
-#if PAN_GPU_INDIRECTS
-        GENX(panfrost_init_indirect_draw_shaders)(dev, &screen->indirect_draw.bin_pool.base);
-#endif
+   struct panfrost_device *dev = &screen->dev;
+
+   screen->vtbl.prepare_shader = prepare_shader;
+   screen->vtbl.screen_destroy = screen_destroy;
+   screen->vtbl.context_populate_vtbl = context_populate_vtbl;
+   screen->vtbl.init_batch = JOBX(init_batch);
+   screen->vtbl.submit_batch = submit_batch;
+   screen->vtbl.get_blend_shader = GENX(pan_blend_get_shader_locked);
+   screen->vtbl.get_compiler_options = GENX(pan_shader_get_compiler_options);
+   screen->vtbl.compile_shader = GENX(pan_shader_compile);
+   screen->vtbl.afbc_size = panfrost_afbc_size;
+   screen->vtbl.afbc_pack = panfrost_afbc_pack;
+
+   GENX(pan_blitter_init)
+   (dev, &screen->blitter.bin_pool.base, &screen->blitter.desc_pool.base);
 }
